{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IR-SQL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Lr4ekZEjU0",
        "colab_type": "text"
      },
      "source": [
        "# Term Proximity Retrieval Model in SQL using DuckDB and MonetDBLite\n",
        "\n",
        "This notebook contains the main code for the Information Retrieval course research project. Two retrievel models (Okapi BM25 and KLD) are implemented in SQL queries that can be run with two database systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzHGaGElGGZ8",
        "colab_type": "text"
      },
      "source": [
        "## Index\n",
        "\n",
        "The Robust04 test collection from TREC is used in this project. The TREC files were indexed using an Anserini indexer. Then the [OldDog](https://github.com/Chriskamphuis/olddog) code from Chris Kamphuis and Arjen de Vries was modified to work with more than one leaf. Their code was further modified to store the collection frequency of each term and the term frequency of each term in each document. The modified code was run on the Anserini index, which resulted in three CSV tables. \n",
        "\n",
        "The *dict* table houses termid, term, document frequency, and collection frequency data. The *docs* table houses name, docid, and document length information. And finally, the *terms* table houses the termid, docid, position, and term frequency data.\n",
        "\n",
        "These three tables are put in an archive, so that they can be easily [downloaded from Dropbox](https://www.dropbox.com/s/bnp788nhpgw0bkb/Robust04Tables.rar?dl=0)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17_-E7XKq737",
        "colab_type": "code",
        "outputId": "5a2ffa70-7622-40e9-b974-68e86fa395e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 691
        }
      },
      "source": [
        "!wget -O Robust04.rar https://www.dropbox.com/s/bnp788nhpgw0bkb/Robust04Tables.rar?dl=0\n",
        "!unrar e -o+ Robust04.rar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-16 11:56:44--  https://www.dropbox.com/s/bnp788nhpgw0bkb/Robust04Tables.rar?dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.1, 2620:100:6016:1::a27d:101\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/bnp788nhpgw0bkb/Robust04Tables.rar [following]\n",
            "--2019-12-16 11:56:50--  https://www.dropbox.com/s/raw/bnp788nhpgw0bkb/Robust04Tables.rar\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com/cd/0/inline/AuUIaL0KYt61-BbktfVGOYdsSrQbpkIC0pAJX5-28KXhRTwGi-8AewLMldwPDvLmscQOX64kOwREKCHGMeG01G9EhPbaTczCtMQ7s-eTOhqqAHtmj4oCQpG_U2gcsplx5Fw/file# [following]\n",
            "--2019-12-16 11:56:50--  https://uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com/cd/0/inline/AuUIaL0KYt61-BbktfVGOYdsSrQbpkIC0pAJX5-28KXhRTwGi-8AewLMldwPDvLmscQOX64kOwREKCHGMeG01G9EhPbaTczCtMQ7s-eTOhqqAHtmj4oCQpG_U2gcsplx5Fw/file\n",
            "Resolving uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com (uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com)... 162.125.8.6, 2620:100:601b:6::a27d:806\n",
            "Connecting to uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com (uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com)|162.125.8.6|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 FOUND\n",
            "Location: /cd/0/inline2/AuXmPSYVKxJ_eYIN58ypFQvCIjvVLQ9c44vc_V6jabgm5dGa2xZaVS_LAE7I_X6_bPJ0Vw-08lKtv4-SZqsY5aCw2PDCd4M28wUoI6w0PS9AeobKGikrz__ZoHZf6zseZl0DiGRjCU8vWkXr1O1CX0nhohTlTz1y5q2jbQO0JkAiwlpp8MaBTYvOioHRr6qYI482zvd5TFyzOsqvobEsTQLv1bS4LOK-7XmsqfHvweOXZLxPNQ7lpMPdICAyWB55algv0GLlulzH-fN3wktEnrnd65NE6DfR5u2pmUd3bHYFek4K9XgtumGINSv5H29vCKoUKSEgg7GomsKN5OIlrVcQ8hJ6fzy-hnXfE9QT-Ln72A/file [following]\n",
            "--2019-12-16 11:56:50--  https://uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com/cd/0/inline2/AuXmPSYVKxJ_eYIN58ypFQvCIjvVLQ9c44vc_V6jabgm5dGa2xZaVS_LAE7I_X6_bPJ0Vw-08lKtv4-SZqsY5aCw2PDCd4M28wUoI6w0PS9AeobKGikrz__ZoHZf6zseZl0DiGRjCU8vWkXr1O1CX0nhohTlTz1y5q2jbQO0JkAiwlpp8MaBTYvOioHRr6qYI482zvd5TFyzOsqvobEsTQLv1bS4LOK-7XmsqfHvweOXZLxPNQ7lpMPdICAyWB55algv0GLlulzH-fN3wktEnrnd65NE6DfR5u2pmUd3bHYFek4K9XgtumGINSv5H29vCKoUKSEgg7GomsKN5OIlrVcQ8hJ6fzy-hnXfE9QT-Ln72A/file\n",
            "Reusing existing connection to uc2a4d6e8638dbfd9371c25538f1.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 724527810 (691M) [application/rar]\n",
            "Saving to: ‘Robust04.rar’\n",
            "\n",
            "Robust04.rar        100%[===================>] 690.96M  60.6MB/s    in 12s     \n",
            "\n",
            "2019-12-16 11:57:02 (59.7 MB/s) - ‘Robust04.rar’ saved [724527810/724527810]\n",
            "\n",
            "\n",
            "UNRAR 5.50 freeware      Copyright (c) 1993-2017 Alexander Roshal\n",
            "\n",
            "\n",
            "Extracting from Robust04.rar\n",
            "\n",
            "Extracting  qrels_binary.csv                                             \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  queries.csv                                                  \b\b\b\b  0%\b\b\b\b\b  OK \n",
            "Extracting  terms.csv                                                    \b\b\b\b  0%\b\b\b\b  1%\b\b\b\b  2%\b\b\b\b  3%\b\b\b\b  4%\b\b\b\b  5%\b\b\b\b  6%\b\b\b\b  7%\b\b\b\b  8%\b\b\b\b  9%\b\b\b\b 10%\b\b\b\b 11%\b\b\b\b 12%\b\b\b\b 13%\b\b\b\b 14%\b\b\b\b 15%\b\b\b\b 16%\b\b\b\b 17%\b\b\b\b 18%\b\b\b\b 19%\b\b\b\b 20%\b\b\b\b 21%\b\b\b\b 22%\b\b\b\b 23%\b\b\b\b 24%\b\b\b\b 25%\b\b\b\b 26%\b\b\b\b 27%\b\b\b\b 28%\b\b\b\b 29%\b\b\b\b 30%\b\b\b\b 31%\b\b\b\b 32%\b\b\b\b 33%\b\b\b\b 34%\b\b\b\b 35%\b\b\b\b 36%\b\b\b\b 37%\b\b\b\b 38%\b\b\b\b 39%\b\b\b\b 40%\b\b\b\b 41%\b\b\b\b 42%\b\b\b\b 43%\b\b\b\b 44%\b\b\b\b 45%\b\b\b\b 46%\b\b\b\b 47%\b\b\b\b 48%\b\b\b\b 49%\b\b\b\b 50%\b\b\b\b 51%\b\b\b\b 52%\b\b\b\b 53%\b\b\b\b 54%\b\b\b\b 55%\b\b\b\b 56%\b\b\b\b 57%\b\b\b\b 58%\b\b\b\b 59%\b\b\b\b 60%\b\b\b\b 61%\b\b\b\b 62%\b\b\b\b 63%\b\b\b\b 64%\b\b\b\b 65%\b\b\b\b 66%\b\b\b\b 67%\b\b\b\b 68%\b\b\b\b 69%\b\b\b\b 70%\b\b\b\b 71%\b\b\b\b 72%\b\b\b\b 73%\b\b\b\b 74%\b\b\b\b 75%\b\b\b\b 76%\b\b\b\b 77%\b\b\b\b 78%\b\b\b\b 79%\b\b\b\b 80%\b\b\b\b 81%\b\b\b\b 82%\b\b\b\b 83%\b\b\b\b 84%\b\b\b\b 85%\b\b\b\b 86%\b\b\b\b 87%\b\b\b\b 88%\b\b\b\b 89%\b\b\b\b 90%\b\b\b\b 91%\b\b\b\b 92%\b\b\b\b 93%\b\b\b\b 94%\b\b\b\b 95%\b\b\b\b 96%\b\b\b\b 97%\b\b\b\b 98%\b\b\b\b\b  OK \n",
            "Extracting  dict.csv                                                     \b\b\b\b 98%\b\b\b\b 99%\b\b\b\b\b  OK \n",
            "Extracting  docs.csv                                                     \b\b\b\b 99%\b\b\b\b\b  OK \n",
            "All OK\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8XbFCaWPzLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXIae-4JC7G",
        "colab_type": "text"
      },
      "source": [
        "## Databases\n",
        "\n",
        "Both DuckDB and MonetDBLite will be used to test the differences between them on a number of dimensions, during the evaluation phase of the research.\n",
        "\n",
        "Google Colab does not pre-install these packages, so that is why we need to do `pip install`. This takes about 5 minutes for DuckDB, so please already run the next cell, before reading on.\n",
        "\n",
        "Here, we have made two classes that do the necessary tasks that we need the databases to do.\n",
        "\n",
        "MonetDB cannot be used, as it requires an external Java MonetDB server, and the project was to be made with the Python APIs. That leaves us with MonetDBLite. For some reason, they have taken out the Python DB API for MonetDBLite (the `Cursor` class and `cursor.execute()` function). We could enable it by modifying the source code, but the program also needs to work for other people on other computers. So we just stuck to the Simple API of MonetDBLite. Another tidbit, MonetDBLite cannot be initialized in the memory. If you go over some memory bandwith threshold, the whole database stops working. So, MonetDBLite needs to be initialized in storage. The performance difference is not that big on Google Colab, but it did matter on a local runtime.\n",
        "\n",
        "When initializing the database objects, the index tables are automatically added. So you only have to initialize them once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFOnPbiYTEph",
        "colab_type": "code",
        "outputId": "006086bb-ccf9-48ff-8f54-2934053e962c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!pip install duckdb\n",
        "import duckdb\n",
        "\n",
        "class DuckDB(object):\n",
        "  \"\"\"\n",
        "  Class that houses all the DuckDB functionalities.\n",
        "\n",
        "  Attributes:\n",
        "    c         = [Cursor] database cursor of DuckDB\n",
        "    C         = [int] number of indexed terms\n",
        "    N         = [int] number of indexed documents\n",
        "    avgdl     = [float] average number of terms per document\n",
        "    len_query = [int] number of terms in current search query\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               database=':memory:',\n",
        "               dict='dict.csv',\n",
        "               docs='docs.csv',\n",
        "               terms='terms.csv'):\n",
        "    \"\"\"\n",
        "    Initializes DuckDB database with index and statistics.\n",
        "\n",
        "    Args:\n",
        "      database = [str] database path\n",
        "      dict     = [str] filename for dictionary CSV\n",
        "      docs     = [str] filename for documents CSV\n",
        "      terms    = [str] filename for terms CSV\n",
        "    \"\"\"\n",
        "    # initialize database\n",
        "    con = duckdb.connect(database)\n",
        "    self.c = con.cursor()\n",
        "\n",
        "    # copy dictionary CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE dict(termid INTEGER \"\n",
        "                                    \",term   VARCHAR \"\n",
        "                                    \",df     INTEGER \"\n",
        "                                    \",cf     INTEGER)\")\n",
        "    self.c.execute(\"COPY dict \"\n",
        "                   \"FROM '\" + dict + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # copy documents CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE docs(name  VARCHAR \"\n",
        "                                    \",docid INTEGER \"\n",
        "                                    \",len   INTEGER \"\n",
        "                                    \",temp  INTEGER)\")\n",
        "    self.c.execute(\"COPY docs \"\n",
        "                   \"FROM '\" + docs + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # copy terms CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE terms(termid INTEGER \"\n",
        "                                     \",docid  INTEGER \"\n",
        "                                     \",pos    INTEGER \"\n",
        "                                     \",tf     INTEGER)\")\n",
        "    self.c.execute(\"COPY terms \"\n",
        "                   \"FROM '\" + terms + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # compute standard index statistics\n",
        "    self.C = self._C()\n",
        "    self.N = self._N()\n",
        "    self.avgdl = self._avgdl()\n",
        "    self.len_query = 0\n",
        "    \n",
        "  def make_query(self, *args: str):\n",
        "    \"\"\"\n",
        "    Makes query table in DuckDB database filled with query terms.\n",
        "    \n",
        "    Args:\n",
        "      args = [[str]] concatenation of strings to be made into a query\n",
        "    \"\"\"\n",
        "    # convert search query in to SQL query\n",
        "    query = \"('\" + args[0] + \"')\"\n",
        "    for arg in args[1:]:\n",
        "        query += \", ('\" + arg + \"')\"\n",
        "    \n",
        "    # make new or replace old query table\n",
        "    self.c.execute(\"DROP TABLE IF EXISTS query\")\n",
        "    self.c.execute(\"CREATE TABLE query(term VARCHAR)\")\n",
        "    self.c.execute(\"INSERT INTO query VALUES \" + query)\n",
        "\n",
        "    # bookkeeping\n",
        "    self.len_query = len(args)\n",
        "\n",
        "  def make_queries(self, queries, qrels):\n",
        "    \"\"\"\n",
        "    Makes queries and qrels tables in DuckDB database filled with\n",
        "    queryid, term pairs and queryid, docid relevance, respectively.\n",
        "\n",
        "    Args:\n",
        "      queries = [str] filename for search queries CSV\n",
        "      qrels   = [str] filename for relevance judgements CSV\n",
        "    \"\"\"\n",
        "    # copy queries CSV into DuckDB database\n",
        "    self.c.execute(\"DROP TABLE IF EXISTS queries\")\n",
        "    self.c.execute(\"CREATE TABLE queries(queryid INTEGER \"\n",
        "                                       \",term    VARCHAR \"\n",
        "                                       \",len     INTEGER)\")\n",
        "    self.c.execute(\"COPY queries \"\n",
        "                   \"FROM '\" + queries + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # copy relevance judgements CSV into DuckDB database\n",
        "    self.c.execute(\"DROP TABLE IF EXISTS qrels\")\n",
        "    self.c.execute(\"CREATE TABLE qrels(queryid INTEGER \"\n",
        "                                     \",name    VARCHAR \"\n",
        "                                     \",rel     INTEGER)\")\n",
        "    self.c.execute(\"COPY qrels \"\n",
        "                   \"FROM '\" + qrels + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "\n",
        "  def execute_query(self, query):\n",
        "    \"\"\"\n",
        "    Executes SQL query on DuckDB database.\n",
        "\n",
        "    Args:\n",
        "      query = [str] the SQL query to be executed by DuckDB\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The output of the execution as a Pandas DataFrame object.\n",
        "    \"\"\"\n",
        "    out = self.c.execute(query)\n",
        "    return out.fetchdf()\n",
        "\n",
        "  def _C(self):\n",
        "    \"\"\" \n",
        "    Gets total number of terms in the index.\n",
        "    \n",
        "    Returns [int]:\n",
        "      Total number of indexed terms.\n",
        "    \"\"\"\n",
        "    C = self.c.execute(\"SELECT SUM(dict.cf) \"\n",
        "                       \"FROM dict\")\n",
        "    return C.fetchdf().iloc[0, 0]\n",
        "\n",
        "  def _N(self):\n",
        "    \"\"\"\n",
        "    Gets number of documents in the index.\n",
        "\n",
        "    Returns [int]:\n",
        "      Number of indexed documents.\n",
        "    \"\"\"\n",
        "    N = self.c.execute(\"SELECT COUNT(*) \"\n",
        "                       \"FROM docs\")\n",
        "    return N.fetchdf().iloc[0, 0]\n",
        "\n",
        "  def _avgdl(self):\n",
        "    \"\"\"\n",
        "    Gets average number of terms per document in the index.\n",
        "\n",
        "    Returns [float]:\n",
        "      Average length of indexed documents.\n",
        "    \"\"\"\n",
        "    avgdl = self.c.execute(\"SELECT AVG(docs.len) \"\n",
        "                           \"FROM docs\")\n",
        "    return avgdl.fetchdf().iloc[0, 0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: duckdb in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from duckdb) (1.17.4)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.6/dist-packages (from duckdb) (0.25.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->duckdb) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23->duckdb) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.23->duckdb) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YojwuWrTd6N",
        "colab_type": "code",
        "outputId": "57729247-e39a-4e53-a9f3-5c079ea5310c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "!pip install monetdblite\n",
        "import monetdblite as m\n",
        "\n",
        "class MonetDBLite(object):\n",
        "  \"\"\" \n",
        "  Class that houses all the MonetDBLite functionalities. \n",
        "\n",
        "  Attributes:\n",
        "    C     = [int] number of indexed terms\n",
        "    N     = [int] number of indexed documents\n",
        "    avgdl = [float] average number of terms per document\n",
        "    len_query = [int] number of terms in current search query\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               database='/tmp/MonetDBLite Database',\n",
        "               dict='dict.csv',\n",
        "               docs='docs.csv',\n",
        "               terms='terms.csv'):\n",
        "    \"\"\"\n",
        "    Initializes MonetDBLite database with index.\n",
        "\n",
        "    Args:\n",
        "      database = [str] database path\n",
        "      dict     = [str] filename for dictionary CSV\n",
        "      docs     = [str] filename for documents CSV\n",
        "      terms    = [str] filename for terms CSV\n",
        "    \"\"\"\n",
        "    # MonetDBLite expects an absolute path\n",
        "    dict = os.path.join('/content', dict)\n",
        "    docs = os.path.join('/content', docs)\n",
        "    terms = os.path.join('/content', terms)\n",
        "\n",
        "    # initialize database\n",
        "    m.init(database)\n",
        "\n",
        "    # copy dictionary CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE dict(termid INTEGER \"\n",
        "                           \",term   VARCHAR(99) \"\n",
        "                           \",df     INTEGER \"\n",
        "                           \",cf     INTEGER)\")\n",
        "    m.sql(\"COPY INTO dict \"\n",
        "          \"FROM '\" + dict + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # copy documents CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE docs(name  VARCHAR(99) \"\n",
        "                           \",docid INTEGER \"\n",
        "                           \",len   INTEGER \"\n",
        "                           \",temp  INTEGER)\") \n",
        "    m.sql(\"COPY INTO docs \"\n",
        "          \"FROM '\" + docs + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # copy terms CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE terms(termid INTEGER \"\n",
        "                            \",docid  INTEGER \"\n",
        "                            \",pos    INTEGER \"\n",
        "                            \",tf     INTEGER)\")\n",
        "    m.sql(\"COPY INTO terms \"\n",
        "          \"FROM '\" + terms + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # compute standard index statistics\n",
        "    self.C = self._C()\n",
        "    self.N = self._N()\n",
        "    self.avgdl = self._avgdl()\n",
        "    self.len_query = 0\n",
        "    \n",
        "  def make_query(self, *args: str):\n",
        "    \"\"\"\n",
        "    Makes query table in MonetDBLite database filled with query terms.\n",
        "    \n",
        "    Args:\n",
        "      args = [[str]] concatenation of strings to be made into a query\n",
        "    \"\"\"\n",
        "    # convert search query in to SQL query\n",
        "    query = \"('\" + args[0] + \"')\"\n",
        "    for arg in args[1:]:\n",
        "        query += \", ('\" + arg + \"')\"\n",
        "    \n",
        "    # make new or replace old query table\n",
        "    m.sql(\"DROP TABLE IF EXISTS query\")\n",
        "    m.sql(\"CREATE TABLE query(term VARCHAR(99))\")\n",
        "    m.sql(\"INSERT INTO query VALUES \" + query)\n",
        "\n",
        "    # bookkeeping\n",
        "    self.len_query = len(args)\n",
        "\n",
        "  def make_queries(self, queries, qrels):\n",
        "    \"\"\"\n",
        "    Makes queries and qrels tables in MonetDBLite database filled with\n",
        "    queryid, term pairs and queryid, docid relevance, respectively.\n",
        "\n",
        "    Args:\n",
        "      queries = [str] filename for search queries CSV\n",
        "      qrels   = [str] filename for relevance judgements CSV\n",
        "    \"\"\"\n",
        "    # MonetDBLite expects an absolute path\n",
        "    queries = os.path.join('/content', queries)\n",
        "    qrels = os.path.join('/content', qrels)\n",
        "\n",
        "    # copy queries CSV into MonetDBLite database\n",
        "    m.sql(\"DROP TABLE IF EXISTS queries\")\n",
        "    m.sql(\"CREATE TABLE queries(queryid INTEGER \"\n",
        "                              \",term    VARCHAR(99) \"\n",
        "                              \",len     INTEGER)\")\n",
        "    m.sql(\"COPY INTO queries \"\n",
        "          \"FROM '\" + queries + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # copy relevance judgements CSV into MonetDBLite database\n",
        "    m.sql(\"DROP TABLE IF EXISTS qrels\")\n",
        "    m.sql(\"CREATE TABLE qrels(queryid INTEGER \"\n",
        "                            \",name    VARCHAR(99) \"\n",
        "                            \",rel     INTEGER)\")\n",
        "    m.sql(\"COPY INTO qrels \"\n",
        "          \"FROM '\" + qrels + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "\n",
        "  def execute_query(self, query):\n",
        "    \"\"\"\n",
        "    Executes SQL query on MonetDBLite database.\n",
        "\n",
        "    Args:\n",
        "      query = [str] the SQL query to be executed by MonetDBLite\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The output of the execution as a Pandas DataFrame object.\n",
        "    \"\"\"\n",
        "    out = m.sql(query)\n",
        "    return pd.DataFrame.from_dict(out)\n",
        "\n",
        "  def _C(self):\n",
        "    \"\"\" \n",
        "    Gets total number of terms in the index.\n",
        "    \n",
        "    Returns [int]:\n",
        "      Total number of indexed terms.\n",
        "    \"\"\"\n",
        "    C = m.sql(\"SELECT SUM(dict.cf) \"\n",
        "              \"FROM dict\")\n",
        "    return pd.DataFrame.from_dict(C).iloc[0, 0]\n",
        "\n",
        "  def _N(self):\n",
        "    \"\"\"\n",
        "    Gets number of documents in the index.\n",
        "\n",
        "    Returns [int]:\n",
        "      Number of indexed documents.\n",
        "    \"\"\"\n",
        "    N = m.sql(\"SELECT COUNT(*) \"\n",
        "              \"FROM docs\")\n",
        "    return pd.DataFrame.from_dict(N).iloc[0, 0]\n",
        "\n",
        "  def _avgdl(self):\n",
        "    \"\"\"\n",
        "    Gets average number of terms per document in the index.\n",
        "\n",
        "    Returns [float]:\n",
        "      Average length of indexed documents.\n",
        "    \"\"\"\n",
        "    avgdl = m.sql(\"SELECT AVG(docs.len) \"\n",
        "                  \"FROM docs\")\n",
        "    return pd.DataFrame.from_dict(avgdl).iloc[0, 0]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting monetdblite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/6a/d49c0b03c62c81098ecd42c6e2ed037979355d00797326a6acd2090f4822/monetdblite-0.6.3-cp36-cp36m-manylinux1_x86_64.whl (7.3MB)\n",
            "\u001b[K     |████████████████████████████████| 7.3MB 4.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.20 in /usr/local/lib/python3.6/dist-packages (from monetdblite) (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from monetdblite) (1.17.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20->monetdblite) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20->monetdblite) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.20->monetdblite) (1.12.0)\n",
            "Installing collected packages: monetdblite\n",
            "Successfully installed monetdblite-0.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3r4KeAvMEh7",
        "colab_type": "text"
      },
      "source": [
        "## Retriever\n",
        "\n",
        "The next class houses the function for retrieving the relevant documents given a number of options, `retrieve()`. The private methods (starting with an underscore) are not meant to be accessed from the outside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p2vSKIYU9H4",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "class Retriever(object):\n",
        "  \"\"\" Class to do document retrieval with term proximity using databases. \"\"\"     \n",
        "  def retrieve(self,\n",
        "               query, \n",
        "               db,\n",
        "               con_query=True, \n",
        "               pre_select='kld', \n",
        "               tp=True,\n",
        "               k=30,\n",
        "               sum=True,\n",
        "               mu=2000,\n",
        "               k1=1.2,\n",
        "               b=0.75,\n",
        "               num_docs=20,\n",
        "               max_span=5,\n",
        "               get_time=False,\n",
        "               verbose=True):\n",
        "    \"\"\"\n",
        "    Function that retreives documents with a Retrieval Status Value (RSV)\n",
        "    based on term-proximity (TP) weighting, Okapi BM25 or Kullback-Leibler\n",
        "    Divergence. When opting for TP, k documents can be pre-selected with\n",
        "    the Okapi BM25 or Kullback-Leibler Divergence retrieval models.\n",
        "\n",
        "    Args:\n",
        "      query      = [[str]] the tokenized and normalied query\n",
        "      db         = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      tp         = [bool] whether to do the term proximity at all\n",
        "      k          = [int] maximum number of documents to retrieve with\n",
        "                         the pre-selection retrieval model\n",
        "      sum        = [bool] whether to sum the pre-selection and term \n",
        "                          proximity scores for the final score\n",
        "      mu         = [float] hyper-parameter for the KLD retrieval model\n",
        "      k1         = [float] hyper-parameter for Okapi BM25\n",
        "      b          = [float] hyper-parameter for Okapi BM25\n",
        "      num_docs   = [int] maximum number of documents to retrieve\n",
        "      max_span   = [int] maximum distance, in number of terms, for a term\n",
        "                         pair to be included in the term proximity score\n",
        "      get_time   = [bool] whether to return the elapsed time with the output\n",
        "      verbose    = [bool] whether to print SQL query and elapsed time\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The Pandas DataFrame output.\n",
        "      When `get_time=True`, tuple (Pandas DataFrame, datetime object).\n",
        "    \"\"\"\n",
        "    # add the search query as a table to the database\n",
        "    db.make_query(*query)\n",
        "\n",
        "    # determine the SQL query\n",
        "    sql = (self._qterms(pre_select, tp) +\n",
        "           self._qtermstf(pre_select, tp) +\n",
        "           self._condocs(db, con_query, tp) +\n",
        "           self._pre_select_subscores(db, con_query, pre_select, mu, k1, b) +\n",
        "           self._topkdocs(pre_select, k) + \n",
        "           self._pairs(con_query, pre_select, tp, max_span) +\n",
        "           self._tpscores(db, tp, k1, b) +\n",
        "           self._scores(pre_select, tp, sum, num_docs))\n",
        "    if verbose:\n",
        "      print('Query: {}'.format(sql))\n",
        "    \n",
        "    # get the elapsed time and the results after executing the SQL query\n",
        "    time = datetime.now()\n",
        "    out = db.execute_query(sql)\n",
        "    time_delta = datetime.now() - time\n",
        "    if verbose:\n",
        "      print('Query time: {}'.format(time_delta))\n",
        "\n",
        "    if get_time:\n",
        "      return out, time_delta\n",
        "    else:\n",
        "      return out\n",
        "\n",
        "  def retrieve_all(self,\n",
        "                   queries,\n",
        "                   qrels,\n",
        "                   db,\n",
        "                   con_query=True, \n",
        "                   pre_select='kld', \n",
        "                   tp=True,\n",
        "                   k=30,\n",
        "                   sum=True,\n",
        "                   mu=2000,\n",
        "                   k1=1.2,\n",
        "                   b=0.75,\n",
        "                   num_docs=20,\n",
        "                   max_span=5):\n",
        "    \"\"\"\n",
        "    Function that retrieves a document ranking for all queries with a\n",
        "    Retrieval Status Value (RSV) based on term-proximity (TP) weighting,\n",
        "    Okapi BM25 or Kullback-Leibler Divergence. When opting for TP, k\n",
        "    documents can be pre-selected with the Okapi BM25 or Kullback-Leibler\n",
        "    Divergence retrieval models. The relevance judgements are also added.\n",
        "\n",
        "    Args:\n",
        "      queries    = [str] filename for search queries CSV\n",
        "      qrels      = [str] filename for relevance judgements CSV\n",
        "      db         = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      tp         = [bool] whether to do the term proximity at all\n",
        "      k          = [int] maximum number of documents to retrieve with\n",
        "                         the pre-selection retrieval model per query\n",
        "      sum        = [bool] whether to sum the pre-selection and term \n",
        "                          proximity scores for the final score\n",
        "      mu         = [float] hyper-parameter for the KLD retrieval model\n",
        "      k1         = [float] hyper-parameter for Okapi BM25\n",
        "      b          = [float] hyper-parameter for Okapi BM25\n",
        "      num_docs   = [int] maximum number of documents to retrieve per query\n",
        "      max_span   = [int] maximum distance, in number of terms, for a term\n",
        "                         pair to be included in the term proximity score\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The Pandas DataFrame output.\n",
        "    \"\"\"\n",
        "    # add the queries and qrels tables to the database\n",
        "    db.make_queries(queries, qrels)\n",
        "\n",
        "    # determine the SQL query\n",
        "    sql = (self._qterms(pre_select, tp, True) +\n",
        "           self._qtermstf(pre_select, tp, True) +\n",
        "           self._condocs(db, con_query, tp, True) +\n",
        "           self._pre_select_subscores(db, con_query, pre_select, mu, k1, b, True) +\n",
        "           self._topkdocs(pre_select, k, True) + \n",
        "           self._pairs(con_query, pre_select, tp, max_span, True) +\n",
        "           self._tpscores(db, tp, k1, b, True) +\n",
        "           self._scores(pre_select, tp, sum, num_docs, True) +\n",
        "           self._qrels())    \n",
        "    print('Query: {}'.format(sql))\n",
        "\n",
        "    # get the elapsed time and the results after executing the SQL query\n",
        "    time = datetime.now()\n",
        "    out = db.execute_query(sql)\n",
        "    time_delta = datetime.now() - time\n",
        "    print('Query time: {}'.format(time_delta))\n",
        "\n",
        "    return out\n",
        "\n",
        "  def nr_relevant_documents(self, queries, qrels, db):\n",
        "    \"\"\"Function that retrieves the number of relevant document for each\n",
        "       search query.\n",
        "\n",
        "    Args:\n",
        "      queries = [str] filename for search queries CSV\n",
        "      qrels   = [str] filename for relevance judgements CSV\n",
        "      db      = [DuckDB|MonetDBLite] database that stores the index\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The Pandas DataFrame output with the queryid and numreldocs columns.\n",
        "    \"\"\"\n",
        "    # add the queries and qrels tables to the database\n",
        "    db.make_queries(queries, qrels)\n",
        "\n",
        "    # determine the SQL query\n",
        "    sql = (\"SELECT qrels.queryid \" +\n",
        "                 \",COUNT(qrels.name) AS numreldocs \"\n",
        "           \"FROM qrels \"\n",
        "           \"GROUP BY qrels.queryid\")\n",
        "    print('Query: {}'.format(sql))\n",
        "\n",
        "    # get the results after executing the SQL query\n",
        "    out = db.execute_query(sql)\n",
        "\n",
        "    return out\n",
        "\n",
        "  def _qterms(self, pre_select, tp, all=False):\n",
        "    \"\"\" \n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to the query terms, including the positional information.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model;\n",
        "                   Kullback-Leibler Divergence retrieval model also needs\n",
        "                   collection frequency information of each term  \n",
        "      tp         = [bool] whether to do the term proximity\n",
        "      all        = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    query = (\"WITH qtermids \"\n",
        "                  \"AS (SELECT dict.termid \"\n",
        "                            \",dict.df \"\n",
        "                            \"{}\"\n",
        "                      \"FROM dict \"\n",
        "                      \"{}\"\n",
        "                      \") \"\n",
        "             \"{}\")\n",
        "    \n",
        "    if tp:\n",
        "      query = query.format(\"{}\",\n",
        "                           \"{}\",\n",
        "                           \", qterms \"\n",
        "                                \"AS (SELECT terms.termid \"\n",
        "                                          \",terms.docid \"\n",
        "                                          \",terms.pos \"\n",
        "                                          \",terms.tf \"\n",
        "                                          \",qtermids.df \"\n",
        "                                          \"{}\"\n",
        "                                    \"FROM terms \"\n",
        "                                    \"JOIN qtermids \"\n",
        "                                    \"ON terms.termid = qtermids.termid\"\n",
        "                                    \") \")\n",
        "      if pre_select == 'kld':\n",
        "        query = query.format(\",dict.cf \"\n",
        "                            \"{}\",\n",
        "                            \"{}\",\n",
        "                            \",qtermids.cf \"\n",
        "                            \"{}\")\n",
        "      \n",
        "      if all:\n",
        "        return query.format(\",queries.queryid \"\n",
        "                            \",queries.len \",\n",
        "                            \"JOIN queries \"\n",
        "                            \"ON dict.term = queries.term\",\n",
        "                            \",qtermids.queryid \"\n",
        "                            \",qtermids.len \")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"JOIN query \"\n",
        "                            \"ON dict.term = query.term\",\n",
        "                            \"\")  \n",
        "    else:\n",
        "      if pre_select == 'kld':\n",
        "        query = query.format(\",dict.cf \"\n",
        "                            \"{}\",\n",
        "                            \"{}\",\n",
        "                            \"{}\")\n",
        "      \n",
        "      if all:\n",
        "        return query.format(\",queries.queryid \"\n",
        "                            \",queries.len \",\n",
        "                            \"JOIN queries \"\n",
        "                            \"ON dict.term = queries.term\",\n",
        "                            \"\")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"JOIN query \"\n",
        "                            \"ON dict.term = query.term\",\n",
        "                            \"\")\n",
        "\n",
        "  def _qtermstf(self, pre_select, tp, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to the query terms, excluding the positional information.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model;\n",
        "                   Kullback-Leibler Divergence retrieval model also needs\n",
        "                   collection frequency information of each term\n",
        "      tp         = [bool] whether to do the term proximity\n",
        "      all        = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'none':\n",
        "      return \"\"\n",
        "    \n",
        "    query = (\", qtermstfrows \"\n",
        "                  \"AS (SELECT qterms.termid \"\n",
        "                            \",qterms.docid \"\n",
        "                            \",qterms.tf \"\n",
        "                            \"{}\"\n",
        "                            \",( ROW_NUMBER() \"\n",
        "                               \"OVER(PARTITION BY {}qterms.termid, qterms.docid \"\n",
        "                                    \"ORDER BY qterms.pos\"\n",
        "                                    \")\"\n",
        "                              \") AS row \"\n",
        "                      \"{}\" \n",
        "                      \") \"\n",
        "             \", qtermstf \"\n",
        "                  \"AS (SELECT qtermstfrows.termid \"\n",
        "                            \",qtermstfrows.docid \"\n",
        "                            \",qtermstfrows.tf \"\n",
        "                            \",qtermstfrows.df \"\n",
        "                            \"{}\"\n",
        "                      \"FROM qtermstfrows \"\n",
        "                      \"WHERE qtermstfrows.row = 1\"\n",
        "                      \") \")\n",
        "\n",
        "    if tp:\n",
        "      if pre_select == 'kld':\n",
        "        query = query.format(\",qterms.df \"\n",
        "                             \",qterms.cf \"\n",
        "                             \"{}\",\n",
        "                             \"{}\",\n",
        "                             \"FROM qterms\",\n",
        "                             \",qtermstfrows.cf \"\n",
        "                             \"{}\")\n",
        "      elif pre_select == 'okapi':\n",
        "        query = query.format(\",qterms.df \"\n",
        "                             \"{}\",\n",
        "                             \"{}\",\n",
        "                             \"FROM qterms\",\n",
        "                             \"{}\")\n",
        "        \n",
        "      if all:\n",
        "        return query.format(\",qterms.queryid \"\n",
        "                            \",qterms.len \",\n",
        "                            \"qterms.queryid ,\",\n",
        "                            \",qtermstfrows.queryid \"\n",
        "                            \",qtermstfrows.len \")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"\",\n",
        "                            \"\")\n",
        "    else:\n",
        "      if pre_select == 'kld':\n",
        "        query = query.format(\",qtermids.df \"\n",
        "                             \",qtermids.cf \"\n",
        "                             \"{}\",\n",
        "                             \"{}\",\n",
        "                             \"FROM terms AS qterms \"\n",
        "                             \"JOIN qtermids \"\n",
        "                             \"ON qterms.termid = qtermids.termid\",\n",
        "                             \",qtermstfrows.cf \"\n",
        "                             \"{}\")  \n",
        "      elif pre_select == 'okapi':\n",
        "        query = query.format(\",qtermids.df \"\n",
        "                             \"{}\",\n",
        "                             \"{}\",\n",
        "                             \"FROM terms AS qterms \"\n",
        "                             \"JOIN qtermids \"\n",
        "                             \"ON qterms.termid = qtermids.termid\",\n",
        "                             \"{}\") \n",
        "         \n",
        "      if all:\n",
        "        return query.format(\",qtermids.queryid \"\n",
        "                            \",qtermids.len \",\n",
        "                            \"qtermids.queryid ,\",\n",
        "                            \",qtermstfrows.queryid \"\n",
        "                            \",qtermstfrows.len \")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"\",\n",
        "                            \"\")\n",
        "\n",
        "  def _condocs(self, db, con_query, tp, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to documents that contain all the query terms.\n",
        "\n",
        "    Args:\n",
        "      db        = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query = [bool] whether all query terms need to be in\n",
        "                         the document for it to be retrieved\n",
        "      tp        = [bool] whether to do the term proximity\n",
        "      all       = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.                         \n",
        "    \"\"\"\n",
        "    if not con_query:\n",
        "      return \"\"\n",
        "      \n",
        "    query = (\", condocs \"\n",
        "                  \"AS (SELECT qterms.docid \"\n",
        "                      \"{}\"\n",
        "                      \"FROM {} AS qterms \"\n",
        "                      \"GROUP BY qterms.docid \"\n",
        "                      \"{}\"\n",
        "                      \"HAVING COUNT(DISTINCT qterms.termid) = {}\"\n",
        "                      \") \")\n",
        "    if tp:\n",
        "      query = query.format(\"{}\",\n",
        "                           \"qterms\",\n",
        "                           \"{}\",\n",
        "                           \"{}\")\n",
        "    else:\n",
        "      query = query.format(\"{}\",\n",
        "                           \"qtermstf\",\n",
        "                           \"{}\",\n",
        "                           \"{}\")\n",
        "      \n",
        "    if all:\n",
        "      return query.format(\",qterms.queryid \",\n",
        "                          \",qterms.queryid \",\n",
        "                          \"MIN(qterms.len)\")\n",
        "    else:\n",
        "      return query.format(\"\",\n",
        "                          \"\",\n",
        "                          db.len_query)\n",
        "\n",
        "  def _pre_select_subscores(self, db, con_query, pre_select, mu, k1, b, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute a score for each \n",
        "    query term-document pair, according to the pre-selection\n",
        "    retrieval model.\n",
        "\n",
        "    Args:\n",
        "      db         = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved     \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      mu         = [float] hyper-parameter for the KLD retrieval model\n",
        "      k1         = [float] hyper-parameter for Okapi BM25\n",
        "      b          = [float] hyper-parameter for Okapi BM25\n",
        "      all        = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "    \n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'none':\n",
        "      return \"\"\n",
        "\n",
        "    if pre_select == 'kld':\n",
        "      query = (\", kldsubscores \"\n",
        "                    \"AS (SELECT qtermstf.docid \"\n",
        "                              \"{}\"\n",
        "                              \",( LOG({:f}+tf*{:f}/cf)\"              \n",
        "                                  \"+\" \n",
        "                                 \"LOG(1/({:f}+docs.len))\"\n",
        "                                \") AS subscore \"\n",
        "                        \"FROM qtermstf \"\n",
        "                        \"{}\"\n",
        "                        \"JOIN docs \"\n",
        "                        \"ON qtermstf.docid = docs.docid\"\n",
        "                        \") \")\n",
        "      query = query.format(\"{}\", mu, db.C, mu, \"{}\")\n",
        "    elif pre_select == 'okapi':\n",
        "      query = (\", okapisubscores \"\n",
        "                    \"AS (SELECT qtermstf.docid \"\n",
        "                              \"{}\"\n",
        "                              \",( LOG(({:f}-df+0.5)/(df+0.5))*tf*({:f}+1)\"\n",
        "                                  \"/\"\n",
        "                                 \"(tf+{:f}*(1-{:f}+{:f}*docs.len/{:f}))\"\n",
        "                                \") AS subscore \"\n",
        "                        \"FROM qtermstf \"\n",
        "                        \"{}\"\n",
        "                        \"JOIN docs \"\n",
        "                        \"ON qtermstf.docid = docs.docid\"\n",
        "                        \") \")\n",
        "      query = query.format(\"{}\", db.N, k1, k1, b, b, db.avgdl, \"{}\")\n",
        "\n",
        "    if con_query:\n",
        "      query = query.format(\"{}\",\n",
        "                           \"JOIN condocs \"\n",
        "                           \"ON qtermstf.docid = condocs.docid \"\n",
        "                           \"{}\")      \n",
        "      if all:\n",
        "        return query.format(\",qtermstf.queryid \",\n",
        "                            \"AND qtermstf.queryid = condocs.queryid \")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"\")\n",
        "    else:\n",
        "      if all:\n",
        "        return query.format(\",qtermstf.queryid \",\n",
        "                            \"\")\n",
        "      else:\n",
        "        return query.format(\"\",\n",
        "                            \"\")\n",
        "\n",
        "  def _topkdocs(self, pre_select, k, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the pre-selection scores,\n",
        "    according to the pre-selection retrieval model, and retrieve\n",
        "    the top k documents.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      k          = [int] maximum number of documents retrieved with\n",
        "                         the pre-selection retrieval model\n",
        "      all        = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'none':\n",
        "      return \"\"\n",
        "\n",
        "    query = (\", topdocs \"\n",
        "                  \"AS (SELECT subscores.docid \"\n",
        "                            \"{}\"\n",
        "                            \",SUM(subscores.subscore) AS score \"\n",
        "                            \",( ROW_NUMBER() \"\n",
        "                                \"OVER({}ORDER BY SUM(subscores.subscore) DESC)\"\n",
        "                              \") AS rank \"\n",
        "                      \"FROM {} AS subscores \"\n",
        "                      \"GROUP BY subscores.docid\"\n",
        "                      \"{}\"\n",
        "                      \") \"\n",
        "             \", topkdocs \"\n",
        "                  \"AS (SELECT topdocs.docid \"\n",
        "                            \"{}\"\n",
        "                            \",topdocs.score \"\n",
        "                      \"FROM topdocs \"\n",
        "                      \"WHERE topdocs.rank BETWEEN 1 AND {}\"\n",
        "                      \") \")\n",
        "    \n",
        "    if all:\n",
        "      query = query.format(\",subscores.queryid \",\n",
        "                           \"PARTITION BY subscores.queryid \",\n",
        "                           \"{}\",\n",
        "                           \" ,subscores.queryid\",\n",
        "                           \",topdocs.queryid \",\n",
        "                           \"{:d}\")\n",
        "    else:\n",
        "      query = query.format(\"\",\n",
        "                           \"\",\n",
        "                           \"{}\",\n",
        "                           \"\",\n",
        "                           \"\",\n",
        "                           \"{:d}\")\n",
        "    \n",
        "    if pre_select == 'kld':\n",
        "      return query.format(\"kldsubscores\",\n",
        "                          k)\n",
        "    elif pre_select == 'okapi':\n",
        "      return query.format(\"okapisubscores\",\n",
        "                          k)\n",
        "\n",
        "  def _pairs(self, con_query, pre_select, tp, max_span, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the term pair instance (tpi) for\n",
        "    each query term pair within a span of max_span terms.\n",
        "\n",
        "    Args:\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved                          \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model  \n",
        "      tp         = [bool] whether to do the term proximity                   \n",
        "      max_span   = [int] the maximum span, in terms, of a term pair to\n",
        "                        include in the term proximity score\n",
        "      all        = [bool] whether to retrieve a document ranking for\n",
        "                          all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if not tp:\n",
        "      return \"\"\n",
        "\n",
        "    query = (\", pairs \"\n",
        "                  \"AS (SELECT qterms1.termid AS termid1 \"\n",
        "                            \",qterms2.termid AS termid2 \"\n",
        "                            \"{}\"\n",
        "                            \",qterms1.docid \"\n",
        "                            \",1.0/(qterms1.pos-qterms2.pos) AS tpi \"\n",
        "                            \",( CASE WHEN qterms1.df > qterms2.df THEN qterms1.df \"\n",
        "                                    \"ELSE qterms2.df \"\n",
        "                               \"END\"\n",
        "                              \") AS maxdf \"\n",
        "                            \",( ROW_NUMBER() \"\n",
        "                               \"OVER(PARTITION BY qterms1.termid \"\n",
        "                                                \",qterms2.termid \"\n",
        "                                                \"{}\"\n",
        "                                                \",qterms1.docid \"\n",
        "                                    \"ORDER BY qterms1.pos)\"\n",
        "                              \") AS row \"\n",
        "                      \"FROM qterms AS qterms1 \"\n",
        "                      \"{}\"\n",
        "                      \"{}\"\n",
        "                      \"JOIN qterms AS qterms2 \"\n",
        "                      \"ON qterms1.docid = qterms2.docid AND \"\n",
        "                         \"{}\"\n",
        "                         \"NOT qterms1.termid = qterms2.termid AND \"\n",
        "                         \"qterms1.pos-qterms2.pos BETWEEN 1 AND {:d}\"\n",
        "                      \") \")\n",
        "    \n",
        "    if all:\n",
        "      if con_query:\n",
        "        query = query.format(\",qterms1.queryid \",\n",
        "                             \",qterms1.queryid \",\n",
        "                             \"JOIN condocs ON qterms1.queryid = condocs.queryid \"\n",
        "                             \"AND qterms1.docid = condocs.docid \",\n",
        "                             \"{}\",\n",
        "                             \"qterms1.queryid = qterms2.queryid AND \",\n",
        "                             max_span)\n",
        "      else:\n",
        "        query = query.format(\",qterms1.queryid \",\n",
        "                             \",qterms1.queryid \",\n",
        "                             \"\",\n",
        "                             \"{}\",\n",
        "                             \"qterms1.queryid = qterms2.queryid AND \",\n",
        "                             max_span)\n",
        "        \n",
        "      if pre_select == 'none':\n",
        "        return query.format(\"\")\n",
        "      else:\n",
        "        return query.format(\"JOIN topkdocs ON qterms1.queryid = topkdocs.queryid \"\n",
        "                            \"AND qterms1.docid = topkdocs.docid \")        \n",
        "    else:\n",
        "      if con_query:\n",
        "        query = query.format(\"\",\n",
        "                             \"\",\n",
        "                             \"JOIN condocs ON qterms1.docid = condocs.docid \",\n",
        "                             \"{}\",\n",
        "                             \"\",\n",
        "                             max_span)\n",
        "      else:\n",
        "        query = query.format(\"\",\n",
        "                             \"\",\n",
        "                             \"\",\n",
        "                             \"{}\",\n",
        "                             \"\",\n",
        "                             max_span)\n",
        "        \n",
        "      if pre_select == 'none':\n",
        "        return query.format(\"\")\n",
        "      else:\n",
        "        return query.format(\"JOIN topkdocs ON qterms1.docid = topkdocs.docid \")\n",
        "      \n",
        "  def _tpscores(self, db, tp, k1, b, all=False):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the term proximity score.\n",
        "\n",
        "    Args:      \n",
        "      db  = [DuckDB|MonetDBLite] database that stores the index\n",
        "      tp  = [bool] whether to do the term proximity\n",
        "      k1  = [float] hyper-parameter for Okapi BM25\n",
        "      b   = [float] hyper-parameter for Okapi BM25\n",
        "      all = [bool] whether to retrieve a document ranking for all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if not tp:\n",
        "      return \"\"\n",
        "      \n",
        "    query = (\", tpisums \"\n",
        "                  \"AS (SELECT pairs.termid1 \"\n",
        "                            \",pairs.termid2 \"\n",
        "                            \",pairs.docid \"\n",
        "                            \"{}\"\n",
        "                            \",SUM(pairs.tpi) AS tpisum \"\n",
        "                      \"FROM pairs \"              \n",
        "                      \"GROUP BY pairs.termid1 \"\n",
        "                              \",pairs.termid2 \"\n",
        "                              \",pairs.docid\"\n",
        "                              \"{}\"\n",
        "                      \") \"\n",
        "            \", tpsubscores \"\n",
        "                  \"AS (SELECT pairs.docid \"\n",
        "                            \"{}\"\n",
        "                            \",( LOG(({:f}-maxdf+0.5)/(maxdf+0.5))*tpisum*({:f}+1)\"\n",
        "                                \"/\"\n",
        "                              \"(tpisum+{:f}*(1-{:f}+{:f}*docs.len/{:f}))\"\n",
        "                              \") AS tpsubscore \"\n",
        "                      \"FROM pairs \"\n",
        "                      \"JOIN tpisums \"\n",
        "                      \"ON pairs.termid1 = tpisums.termid1 AND \"\n",
        "                        \"pairs.termid2 = tpisums.termid2 AND \"\n",
        "                        \"pairs.docid = tpisums.docid \"\n",
        "                        \"{}\"\n",
        "                      \"JOIN docs \"\n",
        "                      \"ON pairs.docid = docs.docid \"\n",
        "                      \"WHERE pairs.row = 1\"\n",
        "                      \") \"\n",
        "            \", tpscores \"\n",
        "                  \"AS (SELECT tpsubscores.docid \"\n",
        "                            \"{}\"\n",
        "                            \",SUM(tpsubscores.tpsubscore) AS tpscore \"\n",
        "                      \"FROM tpsubscores \"\n",
        "                      \"GROUP BY tpsubscores.docid\"\n",
        "                      \"{}\"\n",
        "                      \") \")\n",
        "\n",
        "    if all:\n",
        "      return query.format(\",pairs.queryid \",\n",
        "                          \" ,pairs.queryid\",\n",
        "                          \",pairs.queryid \",\n",
        "                          db.N,\n",
        "                          k1,\n",
        "                          k1,\n",
        "                          b,\n",
        "                          b,\n",
        "                          db.avgdl,\n",
        "                          \"AND pairs.queryid = tpisums.queryid \",\n",
        "                          \",tpsubscores.queryid \",\n",
        "                          \" ,tpsubscores.queryid\")\n",
        "    else:\n",
        "      return query.format(\"\",\n",
        "                          \"\",\n",
        "                          \"\",\n",
        "                          db.N,\n",
        "                          k1,\n",
        "                          k1,\n",
        "                          b,\n",
        "                          b,\n",
        "                          db.avgdl,\n",
        "                          \"\",\n",
        "                          \"\",\n",
        "                          \"\")\n",
        "\n",
        "  def _scores(self, pre_select, tp, sum, num_docs, all=False):\n",
        "    \"\"\" \n",
        "    Get the SQL query that will retrieve or compute the final document scores.\n",
        "\n",
        "    Args:      \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model \n",
        "      tp         = [bool] whether to do the term proximity\n",
        "      sum        = [bool] whether to sum the pre-select and term \n",
        "                          proximity scores for the final score\n",
        "      num_docs   = [int] maximum number of documents to retrieve\n",
        "      all        = [bool] whether to retrieve a document ranking\n",
        "                          for all queries\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if all:\n",
        "      query = (\", scores \"\n",
        "                    \"AS ({}) \"\n",
        "                \", topndocs \"\n",
        "                    \"AS (SELECT scores.queryid \"\n",
        "                              \",scores.docid \"\n",
        "                              \",scores.score \"\n",
        "                              \",scores.rank \"\n",
        "                        \"FROM scores \"\n",
        "                        \"WHERE scores.rank BETWEEN 1 and {:d}\"\n",
        "                        \") \")\n",
        "      \n",
        "      if not tp:\n",
        "        return query.format(\"SELECT topkdocs.docid \"\n",
        "                                  \",topkdocs.queryid \"\n",
        "                                  \",topkdocs.score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(PARTITION BY topkdocs.queryid \"\n",
        "                                          \"ORDER BY topkdocs.score DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM topkdocs\",\n",
        "                            num_docs)\n",
        "      elif pre_select == 'none' or not sum:\n",
        "        return query.format(\"SELECT tpscores.docid \"\n",
        "                                  \",tpscores.queryid \"\n",
        "                                  \",tpscores.tpscore AS score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(PARTITION BY tpscores.queryid \"\n",
        "                                          \"ORDER BY tpscores.tpscore DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM tpscores\",\n",
        "                            num_docs)\n",
        "      else:\n",
        "        return query.format(\"SELECT topkdocs.docid \"\n",
        "                                  \",topkdocs.queryid \"\n",
        "                                  \",( topkdocs.score\"\n",
        "                                      \"+\"\n",
        "                                    \"COALESCE(tpscores.tpscore, 0)\"\n",
        "                                    \") AS score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(PARTITION BY topkdocs.queryid \"\n",
        "                                          \"ORDER BY topkdocs.score\"\n",
        "                                                    \"+\"\n",
        "                                                   \"COALESCE(tpscores.tpscore, 0) \"\n",
        "                                                \"DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM topkdocs \"\n",
        "                            \"LEFT JOIN tpscores \"\n",
        "                            \"ON topkdocs.docid = tpscores.docid AND \"\n",
        "                               \"topkdocs.queryid = tpscores.queryid\",\n",
        "                            num_docs)\n",
        "    else:           \n",
        "      query = (\", scores \"\n",
        "                    \"AS ({}) \"\n",
        "                \"SELECT scores.docid \"\n",
        "                      \",scores.score \"\n",
        "                      \",scores.rank \"\n",
        "                \"FROM scores \"\n",
        "                \"WHERE scores.rank BETWEEN 1 AND {:d}\")\n",
        "      \n",
        "      if not tp:\n",
        "        return query.format(\"SELECT topkdocs.docid \"\n",
        "                                  \",topkdocs.score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(ORDER BY topkdocs.score DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM topkdocs\",\n",
        "                            num_docs)\n",
        "      elif pre_select == 'none' or not sum:\n",
        "        return query.format(\"SELECT tpscores.docid \"\n",
        "                                  \",tpscores.tpscore AS score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(ORDER BY tpscores.tpscore DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM tpscores\",\n",
        "                            num_docs)\n",
        "      else:\n",
        "        return query.format(\"SELECT topkdocs.docid \"\n",
        "                                  \",( topkdocs.score\"\n",
        "                                      \"+\"\n",
        "                                    \"COALESCE(tpscores.tpscore, 0)\"\n",
        "                                    \") AS score \"\n",
        "                                  \",( ROW_NUMBER() \"\n",
        "                                     \"OVER(ORDER BY topkdocs.score\"\n",
        "                                                    \"+\"\n",
        "                                                   \"COALESCE(tpscores.tpscore, 0) \"\n",
        "                                          \"DESC)\"\n",
        "                                    \") AS rank \"\n",
        "                            \"FROM topkdocs \"\n",
        "                            \"LEFT JOIN tpscores \"\n",
        "                            \"ON topkdocs.docid = tpscores.docid\",\n",
        "                            num_docs)\n",
        "        \n",
        "  def _qrels(self):\n",
        "    \"\"\" \n",
        "    Get the SQL query that will retrieve the relevance judgement for each\n",
        "    retrieved query-document pair.\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    query = (\"SELECT topndocs.queryid \"\n",
        "                   \",docs.name \"\n",
        "                   \",topndocs.score \"\n",
        "                   \",topndocs.rank \"\n",
        "                   \",COALESCE(qrels.rel, 0) AS rel \"\n",
        "             \"FROM topndocs \"\n",
        "             \"JOIN docs \"\n",
        "             \"ON topndocs.docid = docs.docid \"\n",
        "             \"LEFT JOIN qrels \"\n",
        "             \"ON topndocs.queryid = qrels.queryid AND \"\n",
        "                \"docs.name = qrels.name\")\n",
        "    return query"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPKySlkvO4-0",
        "colab_type": "text"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "Both databases are initialized, which loads the indices in the databases and makes them ready for execution. This can take about 5 minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0DVuHD8yaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck = DuckDB()\n",
        "monet = MonetDBLite()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXbESd9PoiE",
        "colab_type": "text"
      },
      "source": [
        "The `Retriever` class does not have a constructor, so initializing it is a bit meaningless. It is put in a class to hide the private methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxvqmT_067S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "retriever = Retriever()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N50i-63aGBe1",
        "colab_type": "text"
      },
      "source": [
        "### Running the retriever"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "U0FnZ-rYGAHj",
        "colab": {}
      },
      "source": [
        "duck_rels = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', duck, con_query=False, k=100, num_docs=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5mtDGkaqJSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_rels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltD5O2EfJJiZ",
        "colab_type": "code",
        "outputId": "a061d902-87a6-452f-fe97-8bbaa2f0defd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "monet_bm25 = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', \n",
        "                                    monet, con_query=False, pre_select='okapi', \n",
        "                                    k=100, num_docs=100, tp=False)\n",
        "monet_kld = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', \n",
        "                                    monet, con_query=False, pre_select='kld', \n",
        "                                    k=100, num_docs=100, tp=False)\n",
        "monet_tp = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', \n",
        "                                    monet, con_query=False, pre_select='none', \n",
        "                                    k=100, num_docs=100, tp=True)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: WITH qtermids AS (SELECT dict.termid ,dict.df ,queries.queryid ,queries.len FROM dict JOIN queries ON dict.term = queries.term) , qtermstfrows AS (SELECT qterms.termid ,qterms.docid ,qterms.tf ,qtermids.df ,qtermids.queryid ,qtermids.len ,( ROW_NUMBER() OVER(PARTITION BY qtermids.queryid ,qterms.termid, qterms.docid ORDER BY qterms.pos)) AS row FROM terms AS qterms JOIN qtermids ON qterms.termid = qtermids.termid) , qtermstf AS (SELECT qtermstfrows.termid ,qtermstfrows.docid ,qtermstfrows.tf ,qtermstfrows.df ,qtermstfrows.queryid ,qtermstfrows.len FROM qtermstfrows WHERE qtermstfrows.row = 1) , okapisubscores AS (SELECT qtermstf.docid ,qtermstf.queryid ,( LOG((528030.000000-df+0.5)/(df+0.5))*tf*(1.200000+1)/(tf+1.200000*(1-0.750000+0.750000*docs.len/330.551052))) AS subscore FROM qtermstf JOIN docs ON qtermstf.docid = docs.docid) , topdocs AS (SELECT subscores.docid ,subscores.queryid ,SUM(subscores.subscore) AS score ,( ROW_NUMBER() OVER(PARTITION BY subscores.queryid ORDER BY SUM(subscores.subscore) DESC)) AS rank FROM okapisubscores AS subscores GROUP BY subscores.docid ,subscores.queryid) , topkdocs AS (SELECT topdocs.docid ,topdocs.queryid ,topdocs.score FROM topdocs WHERE topdocs.rank BETWEEN 1 AND 100) , scores AS (SELECT topkdocs.docid ,topkdocs.queryid ,topkdocs.score ,( ROW_NUMBER() OVER(PARTITION BY topkdocs.queryid ORDER BY topkdocs.score DESC)) AS rank FROM topkdocs) , topndocs AS (SELECT scores.queryid ,scores.docid ,scores.score ,scores.rank FROM scores WHERE scores.rank BETWEEN 1 and 100) SELECT topndocs.queryid ,docs.name ,topndocs.score ,topndocs.rank ,COALESCE(qrels.rel, 0) AS rel FROM topndocs JOIN docs ON topndocs.docid = docs.docid LEFT JOIN qrels ON topndocs.queryid = qrels.queryid AND docs.name = qrels.name\n",
            "Query time: 0:00:32.177576\n",
            "Query: WITH qtermids AS (SELECT dict.termid ,dict.df ,dict.cf ,queries.queryid ,queries.len FROM dict JOIN queries ON dict.term = queries.term) , qtermstfrows AS (SELECT qterms.termid ,qterms.docid ,qterms.tf ,qtermids.df ,qtermids.cf ,qtermids.queryid ,qtermids.len ,( ROW_NUMBER() OVER(PARTITION BY qtermids.queryid ,qterms.termid, qterms.docid ORDER BY qterms.pos)) AS row FROM terms AS qterms JOIN qtermids ON qterms.termid = qtermids.termid) , qtermstf AS (SELECT qtermstfrows.termid ,qtermstfrows.docid ,qtermstfrows.tf ,qtermstfrows.df ,qtermstfrows.cf ,qtermstfrows.queryid ,qtermstfrows.len FROM qtermstfrows WHERE qtermstfrows.row = 1) , kldsubscores AS (SELECT qtermstf.docid ,qtermstf.queryid ,( LOG(2000.000000+tf*174540872.000000/cf)+LOG(1/(2000.000000+docs.len))) AS subscore FROM qtermstf JOIN docs ON qtermstf.docid = docs.docid) , topdocs AS (SELECT subscores.docid ,subscores.queryid ,SUM(subscores.subscore) AS score ,( ROW_NUMBER() OVER(PARTITION BY subscores.queryid ORDER BY SUM(subscores.subscore) DESC)) AS rank FROM kldsubscores AS subscores GROUP BY subscores.docid ,subscores.queryid) , topkdocs AS (SELECT topdocs.docid ,topdocs.queryid ,topdocs.score FROM topdocs WHERE topdocs.rank BETWEEN 1 AND 100) , scores AS (SELECT topkdocs.docid ,topkdocs.queryid ,topkdocs.score ,( ROW_NUMBER() OVER(PARTITION BY topkdocs.queryid ORDER BY topkdocs.score DESC)) AS rank FROM topkdocs) , topndocs AS (SELECT scores.queryid ,scores.docid ,scores.score ,scores.rank FROM scores WHERE scores.rank BETWEEN 1 and 100) SELECT topndocs.queryid ,docs.name ,topndocs.score ,topndocs.rank ,COALESCE(qrels.rel, 0) AS rel FROM topndocs JOIN docs ON topndocs.docid = docs.docid LEFT JOIN qrels ON topndocs.queryid = qrels.queryid AND docs.name = qrels.name\n",
            "Query time: 0:00:32.149461\n",
            "Query: WITH qtermids AS (SELECT dict.termid ,dict.df ,queries.queryid ,queries.len FROM dict JOIN queries ON dict.term = queries.term) , qterms AS (SELECT terms.termid ,terms.docid ,terms.pos ,terms.tf ,qtermids.df ,qtermids.queryid ,qtermids.len FROM terms JOIN qtermids ON terms.termid = qtermids.termid) , pairs AS (SELECT qterms1.termid AS termid1 ,qterms2.termid AS termid2 ,qterms1.queryid ,qterms1.docid ,1.0/(qterms1.pos-qterms2.pos) AS tpi ,( CASE WHEN qterms1.df > qterms2.df THEN qterms1.df ELSE qterms2.df END) AS maxdf ,( ROW_NUMBER() OVER(PARTITION BY qterms1.termid ,qterms2.termid ,qterms1.queryid ,qterms1.docid ORDER BY qterms1.pos)) AS row FROM qterms AS qterms1 JOIN qterms AS qterms2 ON qterms1.docid = qterms2.docid AND qterms1.queryid = qterms2.queryid AND NOT qterms1.termid = qterms2.termid AND qterms1.pos-qterms2.pos BETWEEN 1 AND 5) , tpisums AS (SELECT pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid ,SUM(pairs.tpi) AS tpisum FROM pairs GROUP BY pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid) , tpsubscores AS (SELECT pairs.docid ,pairs.queryid ,( LOG((528030.000000-maxdf+0.5)/(maxdf+0.5))*tpisum*(1.200000+1)/(tpisum+1.200000*(1-0.750000+0.750000*docs.len/330.551052))) AS tpsubscore FROM pairs JOIN tpisums ON pairs.termid1 = tpisums.termid1 AND pairs.termid2 = tpisums.termid2 AND pairs.docid = tpisums.docid AND pairs.queryid = tpisums.queryid JOIN docs ON pairs.docid = docs.docid WHERE pairs.row = 1) , tpscores AS (SELECT tpsubscores.docid ,tpsubscores.queryid ,SUM(tpsubscores.tpsubscore) AS tpscore FROM tpsubscores GROUP BY tpsubscores.docid ,tpsubscores.queryid) , scores AS (SELECT tpscores.docid ,tpscores.queryid ,tpscores.tpscore AS score ,( ROW_NUMBER() OVER(PARTITION BY tpscores.queryid ORDER BY tpscores.tpscore DESC)) AS rank FROM tpscores) , topndocs AS (SELECT scores.queryid ,scores.docid ,scores.score ,scores.rank FROM scores WHERE scores.rank BETWEEN 1 and 100) SELECT topndocs.queryid ,docs.name ,topndocs.score ,topndocs.rank ,COALESCE(qrels.rel, 0) AS rel FROM topndocs JOIN docs ON topndocs.docid = docs.docid LEFT JOIN qrels ON topndocs.queryid = qrels.queryid AND docs.name = qrels.name\n",
            "Query time: 0:02:02.649719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9Tvbmo8COaD",
        "colab_type": "code",
        "outputId": "fcd0f42d-a46d-4036-db63-377cff11f54c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "monet_bm25_tp = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', \n",
        "                                    monet, con_query=False, pre_select='okapi', \n",
        "                                    k=100, num_docs=100, tp=True)\n",
        "monet_kld_tp = retriever.retrieve_all('queries.csv', 'qrels_binary.csv', \n",
        "                                    monet, con_query=False, pre_select='kld', \n",
        "                                    k=100, num_docs=100, tp=True)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: WITH qtermids AS (SELECT dict.termid ,dict.df ,queries.queryid ,queries.len FROM dict JOIN queries ON dict.term = queries.term) , qterms AS (SELECT terms.termid ,terms.docid ,terms.pos ,terms.tf ,qtermids.df ,qtermids.queryid ,qtermids.len FROM terms JOIN qtermids ON terms.termid = qtermids.termid) , qtermstfrows AS (SELECT qterms.termid ,qterms.docid ,qterms.tf ,qterms.df ,qterms.queryid ,qterms.len ,( ROW_NUMBER() OVER(PARTITION BY qterms.queryid ,qterms.termid, qterms.docid ORDER BY qterms.pos)) AS row FROM qterms) , qtermstf AS (SELECT qtermstfrows.termid ,qtermstfrows.docid ,qtermstfrows.tf ,qtermstfrows.df ,qtermstfrows.queryid ,qtermstfrows.len FROM qtermstfrows WHERE qtermstfrows.row = 1) , okapisubscores AS (SELECT qtermstf.docid ,qtermstf.queryid ,( LOG((528030.000000-df+0.5)/(df+0.5))*tf*(1.200000+1)/(tf+1.200000*(1-0.750000+0.750000*docs.len/330.551052))) AS subscore FROM qtermstf JOIN docs ON qtermstf.docid = docs.docid) , topdocs AS (SELECT subscores.docid ,subscores.queryid ,SUM(subscores.subscore) AS score ,( ROW_NUMBER() OVER(PARTITION BY subscores.queryid ORDER BY SUM(subscores.subscore) DESC)) AS rank FROM okapisubscores AS subscores GROUP BY subscores.docid ,subscores.queryid) , topkdocs AS (SELECT topdocs.docid ,topdocs.queryid ,topdocs.score FROM topdocs WHERE topdocs.rank BETWEEN 1 AND 100) , pairs AS (SELECT qterms1.termid AS termid1 ,qterms2.termid AS termid2 ,qterms1.queryid ,qterms1.docid ,1.0/(qterms1.pos-qterms2.pos) AS tpi ,( CASE WHEN qterms1.df > qterms2.df THEN qterms1.df ELSE qterms2.df END) AS maxdf ,( ROW_NUMBER() OVER(PARTITION BY qterms1.termid ,qterms2.termid ,qterms1.queryid ,qterms1.docid ORDER BY qterms1.pos)) AS row FROM qterms AS qterms1 JOIN topkdocs ON qterms1.queryid = topkdocs.queryid AND qterms1.docid = topkdocs.docid JOIN qterms AS qterms2 ON qterms1.docid = qterms2.docid AND qterms1.queryid = qterms2.queryid AND NOT qterms1.termid = qterms2.termid AND qterms1.pos-qterms2.pos BETWEEN 1 AND 5) , tpisums AS (SELECT pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid ,SUM(pairs.tpi) AS tpisum FROM pairs GROUP BY pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid) , tpsubscores AS (SELECT pairs.docid ,pairs.queryid ,( LOG((528030.000000-maxdf+0.5)/(maxdf+0.5))*tpisum*(1.200000+1)/(tpisum+1.200000*(1-0.750000+0.750000*docs.len/330.551052))) AS tpsubscore FROM pairs JOIN tpisums ON pairs.termid1 = tpisums.termid1 AND pairs.termid2 = tpisums.termid2 AND pairs.docid = tpisums.docid AND pairs.queryid = tpisums.queryid JOIN docs ON pairs.docid = docs.docid WHERE pairs.row = 1) , tpscores AS (SELECT tpsubscores.docid ,tpsubscores.queryid ,SUM(tpsubscores.tpsubscore) AS tpscore FROM tpsubscores GROUP BY tpsubscores.docid ,tpsubscores.queryid) , scores AS (SELECT topkdocs.docid ,topkdocs.queryid ,( topkdocs.score+COALESCE(tpscores.tpscore, 0)) AS score ,( ROW_NUMBER() OVER(PARTITION BY topkdocs.queryid ORDER BY topkdocs.score+COALESCE(tpscores.tpscore, 0) DESC)) AS rank FROM topkdocs LEFT JOIN tpscores ON topkdocs.docid = tpscores.docid AND topkdocs.queryid = tpscores.queryid) , topndocs AS (SELECT scores.queryid ,scores.docid ,scores.score ,scores.rank FROM scores WHERE scores.rank BETWEEN 1 and 100) SELECT topndocs.queryid ,docs.name ,topndocs.score ,topndocs.rank ,COALESCE(qrels.rel, 0) AS rel FROM topndocs JOIN docs ON topndocs.docid = docs.docid LEFT JOIN qrels ON topndocs.queryid = qrels.queryid AND docs.name = qrels.name\n",
            "Query time: 0:00:55.297921\n",
            "Query: WITH qtermids AS (SELECT dict.termid ,dict.df ,dict.cf ,queries.queryid ,queries.len FROM dict JOIN queries ON dict.term = queries.term) , qterms AS (SELECT terms.termid ,terms.docid ,terms.pos ,terms.tf ,qtermids.df ,qtermids.cf ,qtermids.queryid ,qtermids.len FROM terms JOIN qtermids ON terms.termid = qtermids.termid) , qtermstfrows AS (SELECT qterms.termid ,qterms.docid ,qterms.tf ,qterms.df ,qterms.cf ,qterms.queryid ,qterms.len ,( ROW_NUMBER() OVER(PARTITION BY qterms.queryid ,qterms.termid, qterms.docid ORDER BY qterms.pos)) AS row FROM qterms) , qtermstf AS (SELECT qtermstfrows.termid ,qtermstfrows.docid ,qtermstfrows.tf ,qtermstfrows.df ,qtermstfrows.cf ,qtermstfrows.queryid ,qtermstfrows.len FROM qtermstfrows WHERE qtermstfrows.row = 1) , kldsubscores AS (SELECT qtermstf.docid ,qtermstf.queryid ,( LOG(2000.000000+tf*174540872.000000/cf)+LOG(1/(2000.000000+docs.len))) AS subscore FROM qtermstf JOIN docs ON qtermstf.docid = docs.docid) , topdocs AS (SELECT subscores.docid ,subscores.queryid ,SUM(subscores.subscore) AS score ,( ROW_NUMBER() OVER(PARTITION BY subscores.queryid ORDER BY SUM(subscores.subscore) DESC)) AS rank FROM kldsubscores AS subscores GROUP BY subscores.docid ,subscores.queryid) , topkdocs AS (SELECT topdocs.docid ,topdocs.queryid ,topdocs.score FROM topdocs WHERE topdocs.rank BETWEEN 1 AND 100) , pairs AS (SELECT qterms1.termid AS termid1 ,qterms2.termid AS termid2 ,qterms1.queryid ,qterms1.docid ,1.0/(qterms1.pos-qterms2.pos) AS tpi ,( CASE WHEN qterms1.df > qterms2.df THEN qterms1.df ELSE qterms2.df END) AS maxdf ,( ROW_NUMBER() OVER(PARTITION BY qterms1.termid ,qterms2.termid ,qterms1.queryid ,qterms1.docid ORDER BY qterms1.pos)) AS row FROM qterms AS qterms1 JOIN topkdocs ON qterms1.queryid = topkdocs.queryid AND qterms1.docid = topkdocs.docid JOIN qterms AS qterms2 ON qterms1.docid = qterms2.docid AND qterms1.queryid = qterms2.queryid AND NOT qterms1.termid = qterms2.termid AND qterms1.pos-qterms2.pos BETWEEN 1 AND 5) , tpisums AS (SELECT pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid ,SUM(pairs.tpi) AS tpisum FROM pairs GROUP BY pairs.termid1 ,pairs.termid2 ,pairs.docid ,pairs.queryid) , tpsubscores AS (SELECT pairs.docid ,pairs.queryid ,( LOG((528030.000000-maxdf+0.5)/(maxdf+0.5))*tpisum*(1.200000+1)/(tpisum+1.200000*(1-0.750000+0.750000*docs.len/330.551052))) AS tpsubscore FROM pairs JOIN tpisums ON pairs.termid1 = tpisums.termid1 AND pairs.termid2 = tpisums.termid2 AND pairs.docid = tpisums.docid AND pairs.queryid = tpisums.queryid JOIN docs ON pairs.docid = docs.docid WHERE pairs.row = 1) , tpscores AS (SELECT tpsubscores.docid ,tpsubscores.queryid ,SUM(tpsubscores.tpsubscore) AS tpscore FROM tpsubscores GROUP BY tpsubscores.docid ,tpsubscores.queryid) , scores AS (SELECT topkdocs.docid ,topkdocs.queryid ,( topkdocs.score+COALESCE(tpscores.tpscore, 0)) AS score ,( ROW_NUMBER() OVER(PARTITION BY topkdocs.queryid ORDER BY topkdocs.score+COALESCE(tpscores.tpscore, 0) DESC)) AS rank FROM topkdocs LEFT JOIN tpscores ON topkdocs.docid = tpscores.docid AND topkdocs.queryid = tpscores.queryid) , topndocs AS (SELECT scores.queryid ,scores.docid ,scores.score ,scores.rank FROM scores WHERE scores.rank BETWEEN 1 and 100) SELECT topndocs.queryid ,docs.name ,topndocs.score ,topndocs.rank ,COALESCE(qrels.rel, 0) AS rel FROM topndocs JOIN docs ON topndocs.docid = docs.docid LEFT JOIN qrels ON topndocs.queryid = qrels.queryid AND docs.name = qrels.name\n",
            "Query time: 0:01:05.310875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDGp5RMXsywM",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation framework\n",
        "\n",
        "We can download the custom implementation of the evaluation metrics from our Github repository and import the `metrics.py` Python file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PGmoWnIso6f",
        "colab_type": "code",
        "outputId": "d0d259f6-d7c0-48bc-92d4-c2e98c84e588",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/nnistelrooij/Information-Retrieval/master/metrics.py\n",
        "\n",
        "import metrics\n",
        "import numpy as np"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-16 12:04:44--  https://raw.githubusercontent.com/nnistelrooij/Information-Retrieval/master/metrics.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4626 (4.5K) [text/plain]\n",
            "Saving to: ‘metrics.py’\n",
            "\n",
            "\rmetrics.py            0%[                    ]       0  --.-KB/s               \rmetrics.py          100%[===================>]   4.52K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-12-16 12:04:44 (161 MB/s) - ‘metrics.py’ saved [4626/4626]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ESxDgvwZnKn",
        "colab_type": "text"
      },
      "source": [
        "The following function is a semi-generic function to evaluate a test collection using the specified metric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91OXiTu8ty0u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_test_collection(df, df_rel, metric_func, **kwargs):\n",
        "  \"\"\"\n",
        "  Evaluates the ranked lists from a test collection \n",
        "  of a retrieval model using the specified `metric_func`.\n",
        "\n",
        "  Args:\n",
        "    df          = [DataFrame] pandas DataFrame that houses\n",
        "                              the output of a retrieval model\n",
        "                              (required columns: `queryid`, `rank`, `rel`)\n",
        "    df_rel      = [DataFrame] pandas DataFrame with the number of relevant\n",
        "                              documents per query\n",
        "    metric_func = [function] a function that computes a score\n",
        "                             for a test collection; the function\n",
        "                             accepts one argument, which is a\n",
        "                             nested collection of relevance judgements,\n",
        "                             e.g., [[1,1,0], [0,1,1], [1,0,0], ...], or \n",
        "                             a tuple of a nested collection of relevance \n",
        "                             judgements with the number of relevant documents, \n",
        "                             and optionally a named argument k for depth\n",
        "    **kwargs    = [any] extra arguments for metric_func\n",
        "\n",
        "  Returns [int]:\n",
        "    Score from `metric_func`\n",
        "  \"\"\"\n",
        "  k = kwargs.get('k', None)\n",
        "  query_ids = pd.unique(df.queryid)\n",
        "\n",
        "  rels = []\n",
        "  for query_id in query_ids:\n",
        "    rel = df[df.queryid == query_id].sort_values(by=['rank']).rel.to_numpy()\n",
        "    num_rel = df_rel[df_rel.queryid == query_id].numreldocs.to_numpy()\n",
        "    if k:\n",
        "      rels.append(rel)\n",
        "    else:\n",
        "      rels.append( (rel, num_rel) ) \n",
        "\n",
        "  score = 0\n",
        "  if k:\n",
        "    score = metric_func(rels, k)\n",
        "  else:\n",
        "    score = metric_func(rels)\n",
        "  return score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mkZeU9BLl4N",
        "colab_type": "text"
      },
      "source": [
        "### Model runs MAP and P@k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xCbQMe3xbzr",
        "colab_type": "text"
      },
      "source": [
        "The model runs from earlier are now going to be evaluated on MAP and P@10.\n",
        "\n",
        "MAP needs an argument for the number of relevant documents in the collection. The following cell retrieves the number of relevant documents per query in the test collection."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfJDA_Gldu5w",
        "colab_type": "code",
        "outputId": "c4867818-15af-49ba-be9d-de61b06b5a2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "monet_num_rel_docs = retriever.nr_relevant_documents('queries.csv', 'qrels_binary.csv', monet)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Query: SELECT qrels.queryid ,COUNT(qrels.name) AS numreldocs FROM qrels GROUP BY qrels.queryid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VN5YuOFyd35x",
        "colab_type": "code",
        "outputId": "aaa92f78-73c9-4811-fe75-30dcd25cdc55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(monet_num_rel_docs.queryid.to_numpy().size) # one missing query"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "249\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBGpglc_1U7c",
        "colab_type": "code",
        "outputId": "bdcc9152-2f09-47a2-8ff4-a5a9bb480424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# find missing queries\n",
        "df_queries = pd.read_csv('queries.csv', sep='|', header=None, names=['queryid', 'qterm', 'qlen'])\n",
        "queries_with_rels = pd.unique(monet_num_rel_docs.queryid)\n",
        "df_queries_without_rels = df_queries[~df_queries.queryid.isin(queries_with_rels)]\n",
        "queries_without_rels = pd.unique(df_queries_without_rels.queryid)\n",
        "print(queries_without_rels)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[672]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSE6VaAy4UAy",
        "colab_type": "text"
      },
      "source": [
        "The actual evaluation of the model runs follows hereafter. We remove the queries without relevance judgements, and also add queries that do have relevance judgements but no retrieved documents (i.e., Average Precision of those queries with no results should be 0.0). The latter is achieved by adding one row corresponding to that query and set the relevance to 0, which will yield an Average Precision of 0.0 for that query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQjFBK4Od_VP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_runs = [monet_bm25, monet_kld, monet_tp, monet_bm25_tp, monet_kld_tp]\n",
        "model_names = ['bm25', 'kld', 'tp', 'bm25+tp', 'kld+tp']\n",
        "\n",
        "model_runs_clean = []\n",
        "for run in model_runs:\n",
        "  # remove queries without relevance judgements\n",
        "  run = run[~run.queryid.isin(queries_without_rels)]\n",
        "  # add queries with relevance judgements but without retrieved documents\n",
        "  query_ids = pd.unique(run.queryid)\n",
        "  missing_queries = pd.unique(monet_num_rel_docs[~monet_num_rel_docs.queryid.isin(query_ids)].queryid)\n",
        "  missing_queries_list = []\n",
        "  for query in missing_queries:\n",
        "      row = {}\n",
        "      row.update({'queryid': query})\n",
        "      row.update({'name': ''})\n",
        "      row.update({'score': 0})\n",
        "      row.update({'rank': 0})\n",
        "      row.update({'rel': 0})\n",
        "      missing_queries_list.append(row)\n",
        "  df_missing_queries = pd.DataFrame(missing_queries_list) \n",
        "  run = run.append(df_missing_queries) \n",
        "  model_runs_clean.append(run)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujpy1wxXg6Zj",
        "colab_type": "text"
      },
      "source": [
        "**MAP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmorpSk3ef4i",
        "colab_type": "code",
        "outputId": "16737c8e-0935-4af5-d440-c075138db8a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "for run, name in zip(model_runs_clean, model_names):\n",
        "  nr_of_queries = pd.unique(run.queryid).size  \n",
        "  MAP = evaluate_test_collection(run, monet_num_rel_docs, metrics.mean_average_precision)\n",
        "  print(name + ':', MAP, f'({nr_of_queries} queries)')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bm25: 0.14649987100511055 (249 queries)\n",
            "kld: 0.14861494611205617 (249 queries)\n",
            "tp: 0.08612502460193515 (249 queries)\n",
            "bm25+tp: 0.14694326415675082 (249 queries)\n",
            "kld+tp: 0.1499366919544142 (249 queries)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qneW_EAag_4E",
        "colab_type": "text"
      },
      "source": [
        "**P@k**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UE2ApMX4epKx",
        "colab_type": "code",
        "outputId": "61c36a67-5972-4476-a7e0-7eefdd0144bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "k = 10\n",
        "for run, name in zip(model_runs_clean, model_names):\n",
        "  nr_of_queries = pd.unique(run.queryid).size\n",
        "  precision_at_10 = evaluate_test_collection(run, monet_num_rel_docs, metrics.mean_precision_at_k, k=k)\n",
        "  print(name + ':', precision_at_10, f'({nr_of_queries} queries)')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bm25: 0.3060240963855422 (249 queries)\n",
            "kld: 0.3120481927710843 (249 queries)\n",
            "tp: 0.19277108433734938 (249 queries)\n",
            "bm25+tp: 0.30080321285140565 (249 queries)\n",
            "kld+tp: 0.30803212851405626 (249 queries)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MuGB2VA9rQq",
        "colab_type": "text"
      },
      "source": [
        "### Time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHNMu7LL_u-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from timeit import Timer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV45DI0J-A6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = ['new', 'york', 'city']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-G0DG7HF7xM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_time(query, db, k, pre_select, tp, nr_of_runs=20):\n",
        "  \"\"\"\n",
        "  Runs the query on a database with the specified options\n",
        "  for a number of runs (> 10). The mean time is then computed\n",
        "  over the last 10 runs.\n",
        "\n",
        "  Args:\n",
        "    query      = [[string]] the query to issue in the database\n",
        "    db         = [Object] the database\n",
        "    k          = [int] the number of documents to retrieve by\n",
        "                  the pre-selection model\n",
        "    pre_select = [string] pre-selection model\n",
        "    tp         = [bool] whether to use term proximity\n",
        "    nr_of_runs = [int] the number of runs to execute\n",
        "  \"\"\"\n",
        "  assert nr_of_runs > 10, \"Number of runs should be more than 10\"\n",
        "  times = []\n",
        "  for i in np.arange(nr_of_runs):\n",
        "    _, time = retriever.retrieve(query, db, \n",
        "                                 con_query=False, pre_select=pre_select, \n",
        "                                 k=k, num_docs=k, tp=tp, \n",
        "                                 get_time=True, verbose=False)\n",
        "    times.append(time)\n",
        "  print(\"Min time:\", min(times))\n",
        "  print(\"Max time:\", max(times))\n",
        "  print(\"Mean time:\", np.mean(times[(nr_of_runs-10):]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5GB_IGifHvvx",
        "outputId": "a2d42cef-6ebe-4bab-b2c5-0aa30a399456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"--- Okapi BM25 ---\")\n",
        "run_time(query=query, db=monet, k=1000, pre_select='okapi', tp=False)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Okapi BM25 ---\n",
            "Min time: 0:00:00.681446\n",
            "Max time: 0:00:00.744992\n",
            "Mean time: 0:00:00.700183\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OIyd-tY-FML",
        "colab_type": "code",
        "outputId": "17197d0c-3e20-48c6-8817-6059b32825e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"--- KLD ---\")\n",
        "run_time(query=query, db=monet, k=1000, pre_select='kld', tp=False)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- KLD ---\n",
            "Min time: 0:00:00.636715\n",
            "Max time: 0:00:00.704187\n",
            "Mean time: 0:00:00.669574\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CTf_Et3-Pzd",
        "colab_type": "code",
        "outputId": "f2de7ef4-3dd0-48d0-93c6-39db28cae02b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"--- TP ---\")\n",
        "run_time(query=query, db=monet, k=1000, pre_select='none', tp=True)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- TP ---\n",
            "Min time: 0:00:01.384414\n",
            "Max time: 0:00:01.487231\n",
            "Mean time: 0:00:01.426541\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuJiMkRC-T8y",
        "colab_type": "code",
        "outputId": "c2f63f37-f92d-4616-ab8e-8547a28bea19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"--- Okapi BM25 + TP ---\")\n",
        "run_time(query=query, db=monet, k=1000, pre_select='okapi', tp=True)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- Okapi BM25 + TP ---\n",
            "Min time: 0:00:00.808743\n",
            "Max time: 0:00:00.861124\n",
            "Mean time: 0:00:00.833871\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGsxbFYI-Xtq",
        "colab_type": "code",
        "outputId": "072955fe-8d33-4c87-8642-725a24ab2e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "print(\"--- KLD + TP ---\")\n",
        "run_time(query=query, db=monet, k=1000, pre_select='kld', tp=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- KLD + TP ---\n",
            "Min time: 0:00:00.793035\n",
            "Max time: 0:00:00.862655\n",
            "Mean time: 0:00:00.819320\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDmuTwwQEWHT",
        "colab_type": "text"
      },
      "source": [
        "### Kendall's $\\tau$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T2WnVVp7mUE",
        "colab_type": "text"
      },
      "source": [
        "We can also quantify to what extent ranked lists differ from each other after term proximity using Kendall's $\\tau$. The ranked lists must contain the same elements, i.e, output from the retrieval models should be from term proximity on top *k* documents or just the top *k* documents without term proximity. In the options of the `Retriever` functions we must set `num_docs` equal to `k` to ensure that the returned ranked lists contain the same documents, which is a requirement to compute Kendall's $\\tau$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-ucpHEj3jxi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_ranked_lists(df_A, df_B, query_id):\n",
        "  \"\"\"\n",
        "  Compute Kendall's tau for a specific query from the test collection.\n",
        "  Inputs should be from the output of `Retriever.retrieve_all()`.\n",
        "\n",
        "  Args:\n",
        "    df_A = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `queryid`, `rank`)\n",
        "    df_B = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `queryid`, `rank`)\n",
        "\n",
        "  Returns [tuple]:\n",
        "    Tuple of Kendall's tau and p-value\n",
        "  \"\"\"\n",
        "  ranked_doc_names_A = df_A[df_A.queryid == query_id].sort_values(by=['rank']).name.to_numpy()\n",
        "  ranked_doc_names_B = df_B[df_B.queryid == query_id].sort_values(by=['rank']).name.to_numpy()\n",
        "  return metrics.kendall_tau(ranked_doc_names_A, ranked_doc_names_B)\n",
        "\n",
        "def compare_all_ranked_lists(df_A, df_B):\n",
        "  \"\"\"\n",
        "  Compute Kendall's tau for all queries in a test collection.\n",
        "  Inputs should be from the output of `Retriever.retrieve_all()`.\n",
        "\n",
        "  Args:\n",
        "    df_A = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `queryid`, `rank`)\n",
        "    df_B = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `queryid`, `rank`)\n",
        "\n",
        "    Returns [[tuple]]:\n",
        "      List of tuples in the form (queryid, KendalltauResult)\n",
        "  \"\"\"\n",
        "  query_ids = pd.unique(df_A.queryid)\n",
        "  results = []\n",
        "  for query_id in query_ids:\n",
        "    result = compare_ranked_lists(df_A, df_B, query_id)\n",
        "    results.append((query_id, result))\n",
        "  return results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLGgElfmQskK",
        "colab_type": "text"
      },
      "source": [
        "We can compute the mean Kendall's $\\tau$ for the test collection, but only for BM25 vs BM25+TP, and KLD vs KLD+TP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEhrCvbNP7-T",
        "colab_type": "code",
        "outputId": "d53fa828-7663-41e3-b686-d4294661e824",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "kendall_bm25 = compare_all_ranked_lists(monet_bm25, monet_bm25_tp)\n",
        "taus_bm25 = [res[0] for _, res in kendall_bm25]\n",
        "mean_tau_bm25 = np.mean(taus_bm25)\n",
        "print(\"--- BM25 ---\")\n",
        "print(\"Mean Kendall's tau:\", mean_tau_bm25)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- BM25 ---\n",
            "Mean Kendall's tau: 0.7895357214396996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ya2FO5XDQh01",
        "colab_type": "code",
        "outputId": "2de01358-5f04-447b-bb1a-764a8b886ecb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "kendall_kld = compare_all_ranked_lists(monet_kld, monet_kld_tp)\n",
        "taus_kld = [res[0] for _, res in kendall_kld]\n",
        "mean_tau_kld = np.mean(taus_kld)\n",
        "print(\"--- KLD ---\")\n",
        "print(\"Mean Kendall's tau:\", mean_tau_kld)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- KLD ---\n",
            "Mean Kendall's tau: 0.7520225945713801\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvz3VIpfEsLD",
        "colab_type": "text"
      },
      "source": [
        "**Miscellaneous**\n",
        "\n",
        "We can also do ad hoc retrieval for a single query using `retriever.retrieve()` and compute the Kendall's tau for that query over two ranked lists. Again make sure to set `num_docs` equal to `k`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tuj7GbmBYO4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_ranked_lists_ad_hoc(df_A, df_B):\n",
        "  \"\"\"\n",
        "  Compute Kendall's tau for two ranked lists.\n",
        "  Inputs should be from the output of `Retriever.retrieve()`.\n",
        "\n",
        "  Args:\n",
        "    df_A = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `docid`, `rank`)\n",
        "    df_B = [DataFrame] pandas DataFrame that houses\n",
        "                       the output of a retrieval model\n",
        "                       (required columns: `docid`, `rank`)\n",
        "\n",
        "  Returns [tuple]:\n",
        "    Tuple of Kendall's tau and p-value\n",
        "  \"\"\"\n",
        "  ranked_doc_names_A = df_A.sort_values(by=['rank']).docid.to_numpy()\n",
        "  ranked_doc_names_B = df_B.sort_values(by=['rank']).docid.to_numpy()\n",
        "  return metrics.kendall_tau(ranked_doc_names_A, ranked_doc_names_B)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uvmhehsUxY2",
        "colab_type": "text"
      },
      "source": [
        "## Options\n",
        "\n",
        "Now we will explain how to use this complicated `retrieve()` function by setting a number of specific options on or off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFX-M6HQPlM",
        "colab_type": "text"
      },
      "source": [
        "### All options\n",
        "\n",
        "Let's start with the default options. The query will be (`query=`)*new york* and it will be executed for both databases (`db=DuckDB` and `db=MonetDBLite`).\n",
        "\n",
        "First, the top (`k=`)30 documents are retrieved using the Kullback-Leibler Divergence (`pre_select='kld'`) retrieval model with conjunctive queries (`con_query=True`) and a $\\mu$ of 2000 (`mu=2000`). Then, these 30 documents are scored again with term proximity weighting (`tp=True`) based on a modified version of the [Rasolofo algorithm](https://www.researchgate.net/publication/225174089_Term_Proximity_Scoring_for_Keyword-Based_Retrieval_Systems), with a $k1$ of 1.2 (`k1=1.2`), a $b$ of 0.75 (`b=0.75`), and a maximum distance of 5 (`max_span=5`) between query terms. The scores obtained from KLD and Rasolofo are summed (`sum=True`) to arrive at the final score and ranking. Of this final ranking, 20 documents are retrieved (`num_docs=20`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgVPDEmB4Igm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = ['new', 'york']\n",
        "\n",
        "print(\"DuckDB with all options\")\n",
        "duck_scores = retriever.retrieve(query, duck)\n",
        "\n",
        "print(\"\\nMonetDBLite with all options\")\n",
        "monet_scores = retriever.retrieve(query, monet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er_gH2BcUrG1",
        "colab_type": "text"
      },
      "source": [
        "DuckDB was very slow compared to MonetDBLite (approximately a factor of 35). The query that both databases execute is identical, so the time difference is purely DuckDB's shortcoming. Furthermore, the actual scores from DuckDB are different than from MonetDBLite. However, the ranking, surprisingly, is identical between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nDcip3n5UyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gn2sGid5XyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlRmarebV0X8",
        "colab_type": "text"
      },
      "source": [
        "### No term proximity weighting\n",
        "\n",
        "If you only want to rank the documents based on Okapi BM25, then run the following code. All the other options, where applicable, are still the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK93im7CVw45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with only Okapi BM25\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='okapi', tp=False)\n",
        "\n",
        "print(\"\\nMonetDBLite with  only Okapi BM25\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='okapi', tp=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv6XrXrmW4lJ",
        "colab_type": "text"
      },
      "source": [
        "The difference in time between DuckDB and MonetDBLite is now much smaller (a factor of about 10), but the scores are still different, whereas the ranking is again identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TV9ClY3Wk37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqRqb0iFWlx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VLDFS-AXI1w",
        "colab_type": "text"
      },
      "source": [
        "### Only term proximity weighting\n",
        "\n",
        "If you do not want to pre-select `k` documents, before computing the Rasolofo score, then run the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbzTTWCIXbOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with only Rasolofo\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none')\n",
        "\n",
        "print(\"\\nMonetDBLite with only Rasolofo\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHjlAFSX327",
        "colab_type": "text"
      },
      "source": [
        "The factor in the time difference is approximately 10. The execution time for MonetDBLite is more than previously. This makes sense; the algorithm needs to find the close query term pairs in all the documents instead of only the top 30 documents. This operation has a super-linear running time, so it takes MonetDBLite longer to produce the output.\n",
        "\n",
        "The scores are again different, but now the ranking is also different. So DuckDB does not work for the given query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3a41GsXl-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWj_nAS3Xn3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezy8UUQ1YrhR",
        "colab_type": "text"
      },
      "source": [
        "### Bigger term pair radius\n",
        "\n",
        "If the Rasolofo algorithm retrieves too few documents with the default parameters, the span of a term pair can be expanded to include more documents. To do that, run the below code with the query *wizard hat*.\n",
        "\n",
        "It is slightly slower than the default span with MonetDBLite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RdGwDm3Y7tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = ['wizard', 'hat']\n",
        "\n",
        "print(\"DuckDB with default term pair span\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none')\n",
        "\n",
        "print(\"\\nMonetDBLite with default term pair span\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEYnzs4naAgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ehWfvUzaBca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQhJyibdZI9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with bigger term pair span\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none', max_span=20)\n",
        "\n",
        "print(\"\\nMonetDBLite with bigger term pair span\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none', max_span=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MA5wns7aO2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shbxRlsWaPjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LaK3trkapWz",
        "colab_type": "text"
      },
      "source": [
        "### Only TP scores\n",
        "\n",
        "If you want that the final score is only the Rasolofo score, then set `sum=False`. So not the sum of the pre-selection score and the Rasolofo score as final score, but only the Rasolofo score. If the Rasolofo algorithm could not score a document that was retrieved by the pre-selection retrieval model, then that document will not be included in the ranking. So do be aware of that when setting this option!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P31m40X0bmgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with summed scores\")\n",
        "duck_scores = retriever.retrieve(query, duck)\n",
        "\n",
        "print(\"\\nMonetDBLite with summed scores\")\n",
        "monet_scores = retriever.retrieve(query, monet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDIoEXrfb52u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2zsci_Xb6sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCNajxI1b9op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with Rasolofo scores\")\n",
        "duck_scores = retriever.retrieve(query, duck, sum=False)\n",
        "\n",
        "print(\"\\nMonetDBLite with Rasolofo scores\")\n",
        "monet_scores = retriever.retrieve(query, monet, sum=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ZkxekJcC9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnL9g2NscENC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}