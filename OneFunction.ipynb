{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "OneFunction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5Lr4ekZEjU0",
        "colab_type": "text"
      },
      "source": [
        "# Term Proximity Retrieval Model in SQL using DuckDB and MonetDBLite\n",
        "\n",
        "This notebook is a work in progress for the Information Retrieval course research project. A number of retrievel models are implemented in SQL queries that can be run with two database systems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzHGaGElGGZ8",
        "colab_type": "text"
      },
      "source": [
        "## Index\n",
        "\n",
        "A preliminary index of the Robust04 data set was first [downloaded from Jimmy Lin's Dropbox](https://www.dropbox.com/s/mdoly9sjdalh44x/lucene-index.robust04.pos%2Bdocvectors%2Brawdocs.tar.gz). Then the [OldDog](https://github.com/Chriskamphuis/olddog) code from Chris Kamphuis and Arjen de Vries was modified to work with more than one leaf. Their code was further modified to store the collection frequency of each term and the term frequency of each term in each document. The modified code was run, which resulted in three CSV tables. \n",
        "\n",
        "The *dict* table houses termid, term, document frequency, and collection frequency data. The *docs* table houses name, docid, and document length information. And finally, the *terms* table houses the termid, docid, position, and term frequency data.\n",
        "\n",
        "These three tables are put in an archive, so that they can be easily [downloaded from Dropbox](https://www.dropbox.com/s/5qwq3gn6rto98sd/Robust04%2Bpos%2Btf%2Bcf.rar)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkE-y3cGS1MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget -O Robust04.rar https://dl.dropboxusercontent.com/s/5qwq3gn6rto98sd/Robust04%2Bpos%2Btf%2Bcf.rar?dl=0\n",
        "!unrar e -o+ Robust04.rar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8XbFCaWPzLL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from datetime import datetime"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LXIae-4JC7G",
        "colab_type": "text"
      },
      "source": [
        "## Databases\n",
        "\n",
        "Both DuckDB and MonetDBLite will be used to test the differences between them on a number of dimensions, during the evaluation phase of the research.\n",
        "\n",
        "Google Colab does not pre-install these packages, so that is why we need to do `pip install`. This takes about 5 minutes for DuckDB, so please already run the next cell, before reading on.\n",
        "\n",
        "Here, we have made two classes that do the necessary tasks that we need the databases to do.\n",
        "\n",
        "MonetDB cannot be used, as it requires an external Java MonetDB server, and the project was to be made with the Python APIs. That leaves us with MonetDBLite. For some reason, they have taken out the Python DB API for MonetDBLite (the `Cursor` class and `cursor.execute()` function). We could enable it by modifying the source code, but the program also needs to work for other people on other computers. So we just stuck to the Simple API of MonetDBLite. Another titbit, MonetDBLite cannot be initialized in the memory. If you go over some memory bandwith threshold, the whole database stops working. So, MonetDBLite needs to be initialized in storage. The performance difference is not that big on Google Colab, but it did matter on a local runtime.\n",
        "\n",
        "When initializing the database objects, the index tables are automatically added. So you only have to initialize them once."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFOnPbiYTEph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install duckdb\n",
        "import duckdb\n",
        "\n",
        "class DuckDB(object):\n",
        "  \"\"\"\n",
        "  Class that houses all the DuckDB functionalities.\n",
        "\n",
        "  Attributes:\n",
        "    c         = [Cursor] database cursor of DuckDB\n",
        "    C         = [int] number of indexed terms\n",
        "    N         = [int] number of indexed documents\n",
        "    avgdl     = [float] average number of terms per document\n",
        "    len_query = [int] number of terms in current search query\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               database=':memory:',\n",
        "               dict='dict.csv',\n",
        "               docs='docs.csv',\n",
        "               terms='terms.csv'):\n",
        "    \"\"\"\n",
        "    Initializes DuckDB database with index and statistics.\n",
        "\n",
        "    Args:\n",
        "      database = [str] database path\n",
        "      dict     = [str] filename for dictionary CSV\n",
        "      docs     = [str] filename for documents CSV\n",
        "      terms    = [str] filename for terms CSV\n",
        "    \"\"\"\n",
        "    # initialize database\n",
        "    con = duckdb.connect(database)\n",
        "    self.c = con.cursor()\n",
        "\n",
        "    # copy dictionary CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE dict(termid INTEGER \"\n",
        "                                    \",term   VARCHAR \"\n",
        "                                    \",df     INTEGER \"\n",
        "                                    \",cf     INTEGER)\")\n",
        "    self.c.execute(\"COPY dict \"\n",
        "                   \"FROM '\" + dict + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # copy documents CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE docs(name  VARCHAR \"\n",
        "                                    \",docid INTEGER \"\n",
        "                                    \",len   INTEGER \"\n",
        "                                    \",temp  INTEGER)\")\n",
        "    self.c.execute(\"COPY docs \"\n",
        "                   \"FROM '\" + docs + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # copy terms CSV into DuckDB database\n",
        "    self.c.execute(\"CREATE TABLE terms(termid INTEGER \"\n",
        "                                     \",docid  INTEGER \"\n",
        "                                     \",pos    INTEGER \"\n",
        "                                     \",tf     INTEGER)\")\n",
        "    self.c.execute(\"COPY terms \"\n",
        "                   \"FROM '\" + terms + \"' \"\n",
        "                   \"WITH DELIMITER '|'\")\n",
        "    \n",
        "    # compute standard index statistics\n",
        "    self.C = self._C()\n",
        "    self.N = self._N()\n",
        "    self.avgdl = self._avgdl()\n",
        "    self.len_query = 0\n",
        "    \n",
        "  def make_query(self, *args: str):\n",
        "    \"\"\"\n",
        "    Makes query table in DuckDB database filled with query terms.\n",
        "    \n",
        "    Args:\n",
        "      args = [[str]] concatenation of strings to be made into a query\n",
        "    \"\"\"\n",
        "    # convert search query in to SQL query\n",
        "    query = \"('\" + args[0] + \"')\"\n",
        "    for arg in args[1:]:\n",
        "        query += \", ('\" + arg + \"')\"\n",
        "    \n",
        "    # make new or replace old query table\n",
        "    self.c.execute(\"DROP TABLE IF EXISTS query\")\n",
        "    self.c.execute(\"CREATE TABLE query(term VARCHAR)\")\n",
        "    self.c.execute(\"INSERT INTO query VALUES \" + query)\n",
        "\n",
        "    # bookkeeping\n",
        "    self.len_query = len(args)\n",
        "\n",
        "  def execute_query(self, query):\n",
        "    \"\"\"\n",
        "    Executes SQL query on DuckDB database.\n",
        "\n",
        "    Args:\n",
        "      query = [str] the SQL query to be executed by DuckDB\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The output of the execution as a Pandas DataFrame object.\n",
        "    \"\"\"\n",
        "    out = self.c.execute(query)\n",
        "    return out.fetchdf()\n",
        "\n",
        "  def _C(self):\n",
        "    \"\"\" \n",
        "    Gets total number of terms in the index.\n",
        "    \n",
        "    Returns [int]:\n",
        "      Total number of indexed terms.\n",
        "    \"\"\"\n",
        "    C = self.c.execute(\"SELECT SUM(dict.cf) \"\n",
        "                       \"FROM dict\")\n",
        "    return C.fetchdf().iloc[0, 0]\n",
        "\n",
        "  def _N(self):\n",
        "    \"\"\"\n",
        "    Gets number of documents in the index.\n",
        "\n",
        "    Returns [int]:\n",
        "      Number of indexed documents.\n",
        "    \"\"\"\n",
        "    N = self.c.execute(\"SELECT COUNT(*) \"\n",
        "                       \"FROM docs\")\n",
        "    return N.fetchdf().iloc[0, 0]\n",
        "\n",
        "  def _avgdl(self):\n",
        "    \"\"\"\n",
        "    Gets average number of terms per document in the index.\n",
        "\n",
        "    Returns [float]:\n",
        "      Average length of indexed documents.\n",
        "    \"\"\"\n",
        "    avgdl = self.c.execute(\"SELECT AVG(docs.len) \"\n",
        "                           \"FROM docs\")\n",
        "    return avgdl.fetchdf().iloc[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YojwuWrTd6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install monetdblite\n",
        "import monetdblite as m\n",
        "import pandas as pd\n",
        "\n",
        "class MonetDBLite(object):\n",
        "  \"\"\" \n",
        "  Class that houses all the MonetDBLite functionalities. \n",
        "\n",
        "  Attributes:\n",
        "    C     = [int] number of indexed terms\n",
        "    N     = [int] number of indexed documents\n",
        "    avgdl = [float] average number of terms per document\n",
        "    len_query = [int] number of terms in current search query\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               database='/tmp/MonetDBLite Database',\n",
        "               dict='/content/dict.csv',\n",
        "               docs='/content/docs.csv',\n",
        "               terms='/content/terms.csv'):\n",
        "    \"\"\"\n",
        "    Initializes MonetDBLite database with index.\n",
        "\n",
        "    Args:\n",
        "      database = [str] database path\n",
        "      dict     = [str] filename for dictionary CSV\n",
        "      docs     = [str] filename for documents CSV\n",
        "      terms    = [str] filename for terms CSV\n",
        "    \"\"\"\n",
        "    # initialize database\n",
        "    m.init(database)\n",
        "\n",
        "    # copy dictionary CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE dict(termid INTEGER \"\n",
        "                           \",term   VARCHAR(99) \"\n",
        "                           \",df     INTEGER \"\n",
        "                           \",cf     INTEGER)\")\n",
        "    m.sql(\"COPY INTO dict \"\n",
        "          \"FROM '\" + dict + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # copy documents CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE docs(name  VARCHAR(99) \"\n",
        "                           \",docid INTEGER \"\n",
        "                           \",len   INTEGER \"\n",
        "                           \",temp  INTEGER)\") \n",
        "    m.sql(\"COPY INTO docs \"\n",
        "          \"FROM '\" + docs + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # copy terms CSV into MonetDBLite database\n",
        "    m.sql(\"CREATE TABLE terms(termid INTEGER \"\n",
        "                            \",docid  INTEGER \"\n",
        "                            \",pos    INTEGER \"\n",
        "                            \",tf     INTEGER)\")\n",
        "    m.sql(\"COPY INTO terms \"\n",
        "          \"FROM '\" + terms + \"' \"\n",
        "          \"USING DELIMITERS '|'\")\n",
        "    \n",
        "    # compute standard index statistics\n",
        "    self.C = self._C()\n",
        "    self.N = self._N()\n",
        "    self.avgdl = self._avgdl()\n",
        "    self.len_query = 0\n",
        "    \n",
        "  def make_query(self, *args: str):\n",
        "    \"\"\"\n",
        "    Makes query table in MonetDBLite database filled with query terms.\n",
        "    \n",
        "    Args:\n",
        "      args = [[str]] concatenation of strings to be made into a query\n",
        "    \"\"\"\n",
        "    # convert search query in to SQL query\n",
        "    query = \"('\" + args[0] + \"')\"\n",
        "    for arg in args[1:]:\n",
        "        query += \", ('\" + arg + \"')\"\n",
        "    \n",
        "    # make new or replace old query table\n",
        "    m.sql(\"DROP TABLE IF EXISTS query\")\n",
        "    m.sql(\"CREATE TABLE query(term VARCHAR(99))\")\n",
        "    m.sql(\"INSERT INTO query VALUES \" + query)\n",
        "\n",
        "    # bookkeeping\n",
        "    self.len_query = len(args)\n",
        "\n",
        "  def execute_query(self, query):\n",
        "    \"\"\"\n",
        "    Executes SQL query on MonetDBLite database.\n",
        "\n",
        "    Args:\n",
        "      query = [str] the SQL query to be executed by MonetDBLite\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The output of the execution as a Pandas DataFrame object.\n",
        "    \"\"\"\n",
        "    out = m.sql(query)\n",
        "    return pd.DataFrame.from_dict(out)\n",
        "\n",
        "  def _C(self):\n",
        "    \"\"\" \n",
        "    Gets total number of terms in the index.\n",
        "    \n",
        "    Returns [int]:\n",
        "      Total number of indexed terms.\n",
        "    \"\"\"\n",
        "    C = m.sql(\"SELECT SUM(dict.cf) \"\n",
        "              \"FROM dict\")\n",
        "    return pd.DataFrame.from_dict(C).iloc[0, 0]\n",
        "\n",
        "  def _N(self):\n",
        "    \"\"\"\n",
        "    Gets number of documents in the index.\n",
        "\n",
        "    Returns [int]:\n",
        "      Number of indexed documents.\n",
        "    \"\"\"\n",
        "    N = m.sql(\"SELECT COUNT(*) \"\n",
        "              \"FROM docs\")\n",
        "    return pd.DataFrame.from_dict(N).iloc[0, 0]\n",
        "\n",
        "  def _avgdl(self):\n",
        "    \"\"\"\n",
        "    Gets average number of terms per document in the index.\n",
        "\n",
        "    Returns [float]:\n",
        "      Average length of indexed documents.\n",
        "    \"\"\"\n",
        "    avgdl = m.sql(\"SELECT AVG(docs.len) \"\n",
        "                  \"FROM docs\")\n",
        "    return pd.DataFrame.from_dict(avgdl).iloc[0, 0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d3r4KeAvMEh7",
        "colab_type": "text"
      },
      "source": [
        "## Retriever\n",
        "\n",
        "The next class houses the function for retrieving the relevant documents given a number of options, `retrieve()`. The private methods (starting with an underscore) could really use some help. So please only use the `retrieve()` function further into the file. Perhaps we will clean up the mess later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6p2vSKIYU9H4",
        "colab_type": "code",
        "colab": {},
        "cellView": "both"
      },
      "source": [
        "class Retriever(object):\n",
        "  \"\"\" Class to do document retrieval with term proximity using databases. \"\"\"     \n",
        "  def retrieve(self,\n",
        "               query, \n",
        "               db,\n",
        "               con_query=True, \n",
        "               pre_select='kld', \n",
        "               tp=True,\n",
        "               k=30,\n",
        "               sum=True,\n",
        "               mu=0.8, # totally not sure about this hyper-parameter\n",
        "               k1=1.2,\n",
        "               b=0.75,\n",
        "               num_docs=20,\n",
        "               max_span=5):\n",
        "    \"\"\"\n",
        "    Function that retreives documents with a Retrieval Status Value (RSV)\n",
        "    based on term-proximity (TP) weighting, Okapi BM25 or Kullback-Leibler\n",
        "    Divergence. When opting for TP, k documents can be pre-selected with\n",
        "    the Okapi BM25 or Kullback-Leibler Divergence retrieval models.\n",
        "\n",
        "    Args:\n",
        "      query      = [[str]] the tokenized and normalied query\n",
        "      db         = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      tp         = [bool] whether to do the term proximity at all\n",
        "      k          = [int] maximum number of documents to retrieve with\n",
        "                         the pre-selection retrieval model\n",
        "      sum        = [bool] whether to sum the pre-selection and term \n",
        "                          proximity scores for the final score\n",
        "      mu         = [float] hyper-parameter for the KLD retrieval model\n",
        "      k1         = [float] hyper-parameter for Okapi BM25\n",
        "      b          = [float] hyper-parameter for Okapi BM25\n",
        "      num_docs   = [int] maximum number of documents to retrieve\n",
        "      max_span   = [int] maximum distance, in number of terms, for a term\n",
        "                         pair to be included in the term proximity score\n",
        "\n",
        "    Returns [DataFrame]:\n",
        "      The Pandas DataFrame output.\n",
        "    \"\"\"\n",
        "    # add the search query as a table to the database\n",
        "    db.make_query(*query)\n",
        "\n",
        "    # determine the SQL query\n",
        "    sql = (self._qterms(pre_select, tp) +\n",
        "           self._qtermstf(pre_select, tp) +\n",
        "           self._condocs(db, con_query, tp) +\n",
        "           self._pre_select_subscores(db, con_query, pre_select, mu, k1, b) +\n",
        "           self._topkdocs(pre_select, k) + \n",
        "           self._pairs(con_query, pre_select, tp, max_span) +\n",
        "           self._tpscores(db, tp, k1, b) +\n",
        "           self._scores(pre_select, tp, sum, num_docs))\n",
        "    print('Query: {}'.format(sql))\n",
        "    \n",
        "    # get the elapsed time and the results after executing the SQL query\n",
        "    time = datetime.now()\n",
        "    out = db.execute_query(sql)\n",
        "    time_delta = datetime.now() - time\n",
        "    print('Query time: {}'.format(time_delta))\n",
        "\n",
        "    return out\n",
        "\n",
        "  def _qterms(self, pre_select, tp):\n",
        "    \"\"\" \n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to the query terms, including the positional information.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model;\n",
        "                   Kullback-Leibler Divergence retrieval model also needs\n",
        "                   collection frequency information of each term  \n",
        "      tp         = [bool] whether to do the term proximity\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    query = (\"WITH qtermids \"\n",
        "                  \"AS (SELECT dict.termid \"\n",
        "                            \",dict.df \"\n",
        "                            \"{}\"\n",
        "                      \"FROM dict \"\n",
        "                      \"JOIN query \"\n",
        "                      \"ON dict.term = query.term\"\n",
        "                      \") \"\n",
        "             \"{}\")\n",
        "    \n",
        "    if tp:\n",
        "      query = query.format(\"{}\",\n",
        "                           \", qterms \"\n",
        "                           \"AS (SELECT terms.termid \"\n",
        "                                     \",terms.docid \"\n",
        "                                     \",terms.pos \"\n",
        "                                     \",terms.tf \"\n",
        "                                     \",qtermids.df \"\n",
        "                                     \"{}\"\n",
        "                               \"FROM terms \"\n",
        "                               \"JOIN qtermids \"\n",
        "                               \"ON terms.termid = qtermids.termid\"\n",
        "                               \") \")\n",
        "      if pre_select == 'kld':\n",
        "        return query.format(\",dict.cf \", \",qtermids.cf \")\n",
        "      else:\n",
        "        return query.format(\"\", \"\")\n",
        "    else:\n",
        "      if pre_select == 'kld':\n",
        "        return query.format(\",dict.cf \", \"\")\n",
        "      else:\n",
        "        return query.format(\"\", \"\")  \n",
        "\n",
        "  def _qtermstf(self, pre_select, tp):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to the query terms, excluding the positional information.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model;\n",
        "                   Kullback-Leibler Divergence retrieval model also needs\n",
        "                   collection frequency information of each term\n",
        "      tp         = [bool] whether to do the term proximity\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'none':\n",
        "      return \"\"\n",
        "    \n",
        "    query = (\", qtermstfrows \"\n",
        "                  \"AS (SELECT qterms.termid \"\n",
        "                            \",qterms.docid \"\n",
        "                            \",qterms.tf \"\n",
        "                            \"{}\"\n",
        "                            \"{}\"\n",
        "                            \",( ROW_NUMBER() \"\n",
        "                               \"OVER(PARTITION BY qterms.termid, qterms.docid \"\n",
        "                                    \"ORDER BY qterms.pos\"\n",
        "                                    \")\"\n",
        "                              \") AS row \"\n",
        "                      \"{}\" \n",
        "                      \") \"\n",
        "             \", qtermstf \"\n",
        "                  \"AS (SELECT qtermstfrows.termid \"\n",
        "                            \",qtermstfrows.docid \"\n",
        "                            \",qtermstfrows.tf \"\n",
        "                            \",qtermstfrows.df \"\n",
        "                            \"{}\"\n",
        "                      \"FROM qtermstfrows \"\n",
        "                      \"WHERE qtermstfrows.row = 1\"\n",
        "                      \") \")\n",
        "\n",
        "    if pre_select == 'kld':\n",
        "      if tp:\n",
        "        return query.format(\",qterms.df \",\n",
        "                            \",qterms.cf \",\n",
        "                            \"FROM qterms\",\n",
        "                            \",qtermstfrows.cf \")\n",
        "      else:\n",
        "        return query.format(\",qtermids.df \",\n",
        "                            \",qtermids.cf \",\n",
        "                            \"FROM terms AS qterms \"\n",
        "                            \"JOIN qtermids \"\n",
        "                            \"ON qterms.termid = qtermids.termid\",\n",
        "                            \",qtermstfrows.cf \")\n",
        "    elif pre_select == 'okapi':\n",
        "      if tp:\n",
        "        return query.format(\",qterms.df \",\n",
        "                            \"\",\n",
        "                            \"FROM qterms\",\n",
        "                            \"\")\n",
        "      else:\n",
        "        return query.format(\",qtermids.df \",\n",
        "                            \"\",\n",
        "                            \"FROM terms AS qterms \"\n",
        "                            \"JOIN qtermids \"\n",
        "                            \"ON qterms.termid = qtermids.termid\",\n",
        "                            \"\")\n",
        "\n",
        "  def _condocs(self, db, con_query, tp):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will retrieve the rows in the terms file\n",
        "    belonging to documents that contain all the query terms.\n",
        "\n",
        "    Args:\n",
        "      db        = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query = [bool] whether all query terms need to be in\n",
        "                         the document for it to be retrieved\n",
        "      tp         = [bool] whether to do the term proximity\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.                         \n",
        "    \"\"\"\n",
        "    if con_query:\n",
        "      query = (\", condocs \"\n",
        "                    \"AS (SELECT qterms.docid \"\n",
        "                        \"FROM {} AS qterms \"\n",
        "                        \"GROUP BY qterms.docid \"\n",
        "                        \"HAVING COUNT(DISTINCT qterms.termid) = {:d}\"\n",
        "                        \") \")\n",
        "      if tp:\n",
        "        return query.format(\"qterms\", db.len_query)\n",
        "      else:\n",
        "        return query.format(\"qtermstf\", db.len_query)\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "  def _pre_select_subscores(self, db, con_query, pre_select, mu, k1, b):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute a score for each \n",
        "    query term-document pair, according to the pre-selection\n",
        "    retrieval model.\n",
        "\n",
        "    Args:\n",
        "      db         = [DuckDB|MonetDBLite] database that stores the index\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved     \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      mu         = [float] hyper-parameter for the KLD retrieval model\n",
        "      k1         = [float] hyper-parameter for Okapi BM25\n",
        "      b          = [float] hyper-parameter for Okapi BM25\n",
        "    \n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'kld':\n",
        "      query = (\", kldsubscores \"\n",
        "                    \"AS (SELECT qtermstf.docid \"\n",
        "                              \",( LOG({:f}+tf*{:f}/cf)\"              \n",
        "                                  \"+\" \n",
        "                                 \"LOG(1/({:f}+len))\"\n",
        "                                \") AS subscore \"\n",
        "                        \"FROM qtermstf \"\n",
        "                        \"{}\"\n",
        "                        \"JOIN docs \"\n",
        "                        \"ON qtermstf.docid = docs.docid\"\n",
        "                        \") \")\n",
        "      query = query.format(mu, db.C, mu, \"{}\")\n",
        "    elif pre_select == 'okapi':\n",
        "      query = (\", okapisubscores \"\n",
        "                    \"AS (SELECT qtermstf.docid \"\n",
        "                              \",( LOG(({:f}-df+0.5)/(df+0.5))*tf*({:f}+1)\"\n",
        "                                  \"/\"\n",
        "                                 \"(tf+{:f}*(1-{:f}+{:f}*len/{:f}))\"\n",
        "                                \") AS subscore \"\n",
        "                        \"FROM qtermstf \"\n",
        "                        \"{}\"\n",
        "                        \"JOIN docs \"\n",
        "                        \"ON qtermstf.docid = docs.docid\"\n",
        "                        \") \")\n",
        "      query = query.format(db.N, k1, k1, b, b, db.avgdl, \"{}\")\n",
        "    else:\n",
        "      return \"\"\n",
        "\n",
        "    if con_query:\n",
        "      return query.format(\"JOIN condocs \"\n",
        "                          \"ON qtermstf.docid = condocs.docid \")\n",
        "    else:\n",
        "      return query.format(\"\")\n",
        "\n",
        "  def _topkdocs(self, pre_select, k):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the pre-selection scores,\n",
        "    according to the pre-selection retrieval model, and retrieve\n",
        "    the top k documents.\n",
        "\n",
        "    Args:\n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model\n",
        "      k          = [int] maximum number of documents retrieved with\n",
        "                       the pre-selection retrieval model\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if pre_select == 'none':\n",
        "      return \"\"\n",
        "\n",
        "    query = (\", topdocs \"\n",
        "                  \"AS (SELECT subscores.docid \"\n",
        "                            \",SUM(subscores.subscore) AS score \"\n",
        "                            \",( ROW_NUMBER() \"\n",
        "                                \"OVER(ORDER BY SUM(subscores.subscore) DESC)\"\n",
        "                              \") AS row \"\n",
        "                      \"FROM {} AS subscores \"\n",
        "                      \"GROUP BY subscores.docid\"\n",
        "                      \") \"\n",
        "              \", topkdocs \"\n",
        "                  \"AS (SELECT topdocs.docid \"\n",
        "                            \",topdocs.score \"\n",
        "                      \"FROM topdocs \"\n",
        "                      \"WHERE topdocs.row BETWEEN 1 AND {:d}\"\n",
        "                      \") \")\n",
        "    \n",
        "    if pre_select == 'kld':\n",
        "      return query.format(\"kldsubscores\", k)\n",
        "    elif pre_select == 'okapi':\n",
        "      return query.format(\"okapisubscores\", k)\n",
        "\n",
        "  def _pairs(self, con_query, pre_select, tp, max_span):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the term pair instance (tpi) for\n",
        "    each query term pair within a span of max_span terms.\n",
        "\n",
        "    Args:\n",
        "      con_query  = [bool] whether all query terms need to be in\n",
        "                          the document for it to be retrieved                          \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model  \n",
        "      tp         = [bool] whether to do the term proximity                   \n",
        "      max_span   = [int] the maximum span, in terms, of a term pair to\n",
        "                        include in the term proximity score\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if not tp:\n",
        "      return \"\"\n",
        "\n",
        "    query = (\", pairs \"\n",
        "                  \"AS (SELECT qterms1.termid AS termid1 \"\n",
        "                            \",qterms2.termid AS termid2 \"\n",
        "                            \",qterms1.docid \"\n",
        "                            \",1.0/(qterms1.pos-qterms2.pos) AS tpi \"\n",
        "                            \",( CASE WHEN qterms1.df > qterms2.df THEN qterms1.df \"\n",
        "                                    \"ELSE qterms2.df \"\n",
        "                               \"END\"\n",
        "                              \") AS maxdf \"\n",
        "                      \"FROM qterms AS qterms1 \"\n",
        "                      \"{}\"\n",
        "                      \"{}\"\n",
        "                      \"JOIN qterms AS qterms2 \"\n",
        "                      \"ON qterms1.docid = qterms2.docid AND \"\n",
        "                         \"NOT qterms1.termid = qterms2.termid AND \"\n",
        "                         \"qterms1.pos - qterms2.pos BETWEEN 1 AND {:d}\"\n",
        "                      \") \")\n",
        "    if con_query:\n",
        "      query = query.format(\"JOIN condocs ON qterms1.docid = condocs.docid \",\n",
        "                           \"{}\",\n",
        "                           max_span)\n",
        "    else:\n",
        "      query = query.format(\"\",\n",
        "                           \"{}\",\n",
        "                           max_span)\n",
        "      \n",
        "    if pre_select == 'none':\n",
        "      return query.format(\"\")\n",
        "    else:\n",
        "      return query.format(\"JOIN topkdocs ON qterms1.docid = topkdocs.docid \")\n",
        "      \n",
        "  def _tpscores(self, db, tp, k1, b):\n",
        "    \"\"\"\n",
        "    Get the SQL query that will compute the term proximity score.\n",
        "\n",
        "    Args:      \n",
        "      db        = [DuckDB|MonetDBLite] database that stores the index\n",
        "      tp       = [bool] whether to do the term proximity\n",
        "      k1       = [float] hyper-parameter for Okapi BM25\n",
        "      b        = [float] hyper-parameter for Okapi BM25\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    if not tp:\n",
        "      return \"\"\n",
        "      \n",
        "    query = (\",tpisums \"\n",
        "                  \"AS (SELECT pairs.termid1 \"\n",
        "                            \",pairs.termid2 \"\n",
        "                            \",pairs.docid \"\n",
        "                            \",SUM(pairs.tpi) AS tpisum \"\n",
        "                      \"FROM pairs \"              \n",
        "                      \"GROUP BY pairs.termid1 \"\n",
        "                              \",pairs.termid2 \"\n",
        "                              \",pairs.docid\"\n",
        "                      \") \"\n",
        "            \", tpsubscores \"\n",
        "                  \"AS (SELECT pairs.docid \"\n",
        "                            \",( LOG(({:f}-maxdf+0.5)/(maxdf+0.5))*tpisum*({:f}+1)\"\n",
        "                                \"/\"\n",
        "                              \"(tpisum+{:f}*(1-{:f}+{:f}*len/{:f}))\"\n",
        "                              \") AS tpsubscore \"\n",
        "                      \"FROM pairs \"\n",
        "                      \"JOIN tpisums \"\n",
        "                      \"ON pairs.termid1 = tpisums.termid1 AND \"\n",
        "                        \"pairs.termid2 = tpisums.termid2 AND \"\n",
        "                        \"pairs.docid = tpisums.docid \"\n",
        "                      \"JOIN docs \"\n",
        "                      \"ON pairs.docid = docs.docid\"\n",
        "                      \") \"\n",
        "            \", tpscores \"\n",
        "                  \"AS (SELECT tpsubscores.docid \"\n",
        "                            \",SUM(tpsubscores.tpsubscore) AS tpscore \"\n",
        "                      \"FROM tpsubscores \"\n",
        "                      \"GROUP BY tpsubscores.docid\"\n",
        "                      \") \")\n",
        "    return query.format(db.N, k1, k1, b, b, db.avgdl)\n",
        "\n",
        "  def _scores(self, pre_select, tp, sum, num_docs):\n",
        "    \"\"\" \n",
        "    Get the SQL query that will retrieve or compute the final document scores.\n",
        "\n",
        "    Args:      \n",
        "      pre_select = ['kld'|'okapi'|'none'] pre-selection retrieval model \n",
        "      tp         = [bool] whether to do the term proximity\n",
        "      sum        = [bool] whether to sum the pre-select and term \n",
        "                          proximity scores for the final score\n",
        "      num_docs   = [int] maximum number of documents to retrieve\n",
        "\n",
        "    Returns [str]:\n",
        "      SQL query as string.\n",
        "    \"\"\"\n",
        "    query = (\", scores \"\n",
        "                  \"AS ({}) \"\n",
        "             \"SELECT scores.docid \"\n",
        "                   \",scores.score \"\n",
        "             \"FROM scores \"\n",
        "             \"ORDER BY scores.score DESC \"\n",
        "             \"LIMIT {:d}\")\n",
        "    \n",
        "    if not tp:\n",
        "      return query.format(\"SELECT topkdocs.docid \"\n",
        "                                \",topkdocs.score \"\n",
        "                          \"FROM topkdocs\",\n",
        "                          num_docs)\n",
        "    elif pre_select == 'none' or not sum:\n",
        "      return query.format(\"SELECT tpscores.docid \"\n",
        "                                \",tpscores.tpscore AS score \"\n",
        "                          \"FROM tpscores\",\n",
        "                          num_docs)\n",
        "    else:\n",
        "      return query.format(\"SELECT topkdocs.docid \"\n",
        "                                \",( topkdocs.score\"\n",
        "                                    \"+\"\n",
        "                                   \"COALESCE(tpscores.tpscore, 0)\"\n",
        "                                  \") AS score \"\n",
        "                          \"FROM topkdocs \"\n",
        "                          \"LEFT JOIN tpscores \"\n",
        "                          \"ON topkdocs.docid = tpscores.docid\",\n",
        "                          num_docs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPKySlkvO4-0",
        "colab_type": "text"
      },
      "source": [
        "## Initialization\n",
        "\n",
        "Both databases are initialized, which loads the indices in the databases and makes them ready for execution. This can take about 5 minutes, so time for coffee!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH0DVuHD8yaR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck = DuckDB()\n",
        "monet = MonetDBLite()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXXbESd9PoiE",
        "colab_type": "text"
      },
      "source": [
        "The `Retriever` class does not have a constructor, so initializing it is a bit meaningless. I still prefer to put it in a class to hide the private methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxvqmT_067S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "retriever = Retriever()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1uvmhehsUxY2",
        "colab_type": "text"
      },
      "source": [
        "## Options\n",
        "\n",
        "Now we will explain how to use this complicated `retrieve()` function by setting a number of specific options on or off."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRFX-M6HQPlM",
        "colab_type": "text"
      },
      "source": [
        "### All options\n",
        "\n",
        "Let's start with the default options. The query will be (`query=`)*new york* and it will be executed for both databases (`db=DuckDB` and `db=MonetDBLite`).\n",
        "\n",
        "First, the top (`k=`)30 documents are retrieved using the Kullback-Leibler Divergence (`pre_select='kld'`) retrieval model with conjunctive queries (`con_query=True`) and a $\\mu$ of 0.8 (`mu=0.8`). Then, these 30 documents are scored again with term proximity weighting (`tp=True`) based on a modified version of the [Rasolofo algorithm](https://www.researchgate.net/publication/225174089_Term_Proximity_Scoring_for_Keyword-Based_Retrieval_Systems), with a $k1$ of 1.2 (`k1=1.2`), a $b$ of 0.75 (`b=0.75`), and a maximum distance of 5 (`max_span=5`) between query terms. The scores obtained from KLD and Rasolofo are summed (`sum=True`) to arrive at the final score and ranking. Of this final ranking, 20 documents are retrieved (`num_docs=20`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgVPDEmB4Igm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = ['new', 'york']\n",
        "\n",
        "print(\"DuckDB with all options\")\n",
        "duck_scores = retriever.retrieve(query, duck)\n",
        "\n",
        "print(\"\\nMonetDBLite with all options\")\n",
        "monet_scores = retriever.retrieve(query, monet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er_gH2BcUrG1",
        "colab_type": "text"
      },
      "source": [
        "DuckDB was very slow compared to MonetDBLite (approximately a factor of 35). The query that both databases execute is identical, so the time difference is purely DuckDB's shortcoming. Furthermore, the actual scores from DuckDB are different than from MonetDBLite. However, the ranking, surprisingly, is identical between the two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nDcip3n5UyA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gn2sGid5XyU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlRmarebV0X8",
        "colab_type": "text"
      },
      "source": [
        "### No term proximity weighting\n",
        "\n",
        "If you only want to rank the documents based on Okapi BM25, then run the following code. All the other options, where applicable, are still the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sK93im7CVw45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with only Okapi BM25\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='okapi', tp=False)\n",
        "\n",
        "print(\"\\nMonetDBLite with  only Okapi BM25\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='okapi', tp=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv6XrXrmW4lJ",
        "colab_type": "text"
      },
      "source": [
        "The difference in time between DuckDB and MonetDBLite is now much smaller (a factor of about 10), but the scores are still different, whereas the ranking is again identical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TV9ClY3Wk37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqRqb0iFWlx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VLDFS-AXI1w",
        "colab_type": "text"
      },
      "source": [
        "### Only term proximity weighting\n",
        "\n",
        "If you do not want to pre-select `k` documents, before computing the Rasolofo score, then run the following code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbzTTWCIXbOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with only Rasolofo\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none')\n",
        "\n",
        "print(\"\\nMonetDBLite with only Rasolofo\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzHjlAFSX327",
        "colab_type": "text"
      },
      "source": [
        "The factor in the time difference is approximately 10. The execution time for MonetDBLite is more than previously. This makes sense; the algorithm needs to find the close query term pairs in all the documents instead of only the top 30 documents. This operation has a super-linear running time, so it takes MonetDBLite longer to produce the output.\n",
        "\n",
        "The scores are again different, but now the ranking is also different. So DuckDB does not work for the given query."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N3a41GsXl-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWj_nAS3Xn3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ezy8UUQ1YrhR",
        "colab_type": "text"
      },
      "source": [
        "### Bigger term pair radius\n",
        "\n",
        "If the Rasolofo algorithm retrieves too few documents with the default parameters, the span of a term pair can be expanded to include more documents. To do that, run the below code with the query *wizard hat*.\n",
        "\n",
        "It is slightly slower than the default span with MonetDBLite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9RdGwDm3Y7tM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = ['wizard', 'hat']\n",
        "\n",
        "print(\"DuckDB with default term pair span\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none')\n",
        "\n",
        "print(\"\\nMonetDBLite with default term pair span\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEYnzs4naAgP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ehWfvUzaBca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQhJyibdZI9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with bigger term pair span\")\n",
        "duck_scores = retriever.retrieve(query, duck, pre_select='none', max_span=20)\n",
        "\n",
        "print(\"\\nMonetDBLite with bigger term pair span\")\n",
        "monet_scores = retriever.retrieve(query, monet, pre_select='none', max_span=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MA5wns7aO2b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shbxRlsWaPjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LaK3trkapWz",
        "colab_type": "text"
      },
      "source": [
        "### Only TP scores\n",
        "\n",
        "If you want that the final score is only the Rasolofo score, then set `sum=False`. So not the sum of the pre-selection score and the Rasolofo score as final score, but only the Rasolofo score. If the Rasolofo algorithm could not score a document that was retrieved by the pre-selection retrieval model, then that document will not be included in the ranking. So do be aware of that when setting this option!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P31m40X0bmgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with summed scores\")\n",
        "duck_scores = retriever.retrieve(query, duck)\n",
        "\n",
        "print(\"\\nMonetDBLite with summed scores\")\n",
        "monet_scores = retriever.retrieve(query, monet)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDIoEXrfb52u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2zsci_Xb6sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCNajxI1b9op",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"DuckDB with summed scores\")\n",
        "duck_scores = retriever.retrieve(query, duck, sum=False)\n",
        "\n",
        "print(\"\\nMonetDBLite with summed scores\")\n",
        "monet_scores = retriever.retrieve(query, monet, sum=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ZkxekJcC9M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "duck_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnL9g2NscENC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "monet_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxId-FH-cQ43",
        "colab_type": "text"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "As seen by the many options, there are endless opportunities for research and evaluation. And we haven't even talked about combining options, disjunctive queries, or hyper-parameters, yet.\n",
        "\n",
        "Our question right now is basically: why is DuckDB so slow and why does DuckDB give incorrect and unintuitive results. Analyzing the runtime of MonetDBLite was much more straightforward than for DuckDB. Is there still some hope for DuckDB, maybe in the future? Or should we only focus on MonetDBLite and maybe implement a second term proximity retrieval model, instead of comparing the two databases?"
      ]
    }
  ]
}